import os
import sys
import glob
import numpy as np
import pandas as pd
import math
import sys
import random
import pickle
import csv

import dask.dataframe as dd
import dask.array as da
from dask.distributed import Client

import torch
import pyro
import pyro.distributions as dist
import pyro.distributions.constraints as constraints
from pyro.nn import PyroModule

from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

sys.path.insert(0, '/home/djl34/lab_pd/kl/git/KL/scripts')
import raklette
from run_raklette import run_raklette
from run_raklette import run_raklette_cov
from run_raklette import TSVDataset
# from snakefile_raklette import snakefile_raklette_cov
# from snakefile_raklette import snakefile_make_sample

sys.path.insert(0, '/home/djl34/lab_pd/simulator/code')
from others import round_sig


factor = 1

def get_mem_mb(wildcards, attempt):
    return attempt * 20000 * factor

def get_mem_mb_sample(wildcards, attempt):
    return attempt * 40000 * factor

def get_mem_mb_length(wildcards, attempt):
    return attempt * 5000 * factor

def get_mem_mb_chunks(wildcards, attempt):
    return attempt * int(wildcards.chunksize)/1500 * factor

def get_mem_mb_raklette(wildcards, attempt):
    return attempt* int(wildcards.chunksize)/500 * factor

###################################################################################################################

KL_data_dir = "/home/djl34/lab_pd/kl/data"
scratch_dir = "/n/scratch3/users/d/djl34"

base_set = ["A", "C", "T", "G"]
chrom_set = [str(x) for x in range(1, 23)]
chrom_set = ["2"]
# chrom_set = ["-2"]
# chrom_set = ["22"]


even_chrom_set = [str(2 * x) for x in range(1, 12)]
# chrom_set = even_chrom_set

# cutoff_list = [0, 1, 2, 3, 4, 5]

wildcard_constraints:
    chrom="\d+",
    chrom_all="[-+]?\d+",
    epoch="\d+",
    interval_min="[+-]?([0-9]*[.])?[0-9]+",
    interval_max="[+-]?([0-9]*[.])?[0-9]+",
    samplesize="\d+",

###################################################################################################################

rule sample_size:
    input:
        os.path.join(scratch_dir, "kl_input/"+ "{header}_chr_{chrom}.tsv")
    output:
        first = os.path.join(scratch_dir, "kl_input/"+ "{header}_sample_{samplesize}_chr_{chrom}_sampled.tsv"),
        second = os.path.join(scratch_dir, "kl_input/"+ "{header}_sample_{samplesize}_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb_sample
    run:
        with Client() as client:
            sample_size = wildcards.samplesize
            output_first_filename = output.first
            output_second_filename = output.second
            input_filename = input[0]
            
            rate = dd.read_csv(input_filename, sep = "\t")

            sites = len(rate)

            if sites != 0:
                rate = rate.sample(frac = int(sample_size)/sites, replace = True)

            rate.to_csv(output_first_filename, sep = "\t", index = None, single_file = True)

            head_value = int(wildcards.samplesize) + 1
            
            shell("head -n {head_value} " + output_first_filename + " >> " + output_second_filename)

            
rule make_chunks:
    input:
        os.path.join(scratch_dir, "kl_input/"+ "{header}_chr_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/"+ "{header}_chr_{chrom}/chunk_{chunksize}_0.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=get_mem_mb_chunks
    run:
        #remove previous files
        header = "_".join(output[0].split("_")[:-1])
        header = header + "*.tsv"
        try:
            shell("rm " + header)
        except:
            print("no file")
        print("removing " + header)
                
        iterator = 0
        
        input_filename = input[0]
        output_dir = "/".join(output[0].split("/")[:-1])
        
        chunksize = int(wildcards.chunksize)
        
        with pd.read_csv(input_filename, chunksize=chunksize, sep = "\t") as reader:
            row = 0
            for chunk in reader:
                print(iterator)
                chunk.to_csv(output_dir + "/chunk_" + str(chunksize) + "_" + str(iterator) + ".tsv", sep = "\t", index = None)
                iterator += 1
                           
rule make_tsv_files_even:
    input:
        [os.path.join(scratch_dir, "kl_input/" +"{header}_chr_" + chrom + "/chunk_{chunksize}_0.tsv") for chrom in even_chrom_set]
    output:
        os.path.join(scratch_dir, "kl_input/" + "{header}_chr_-2/chunk_{chunksize}_0.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=get_mem_mb_chunks
    run:
        #remove previous files
        header = "_".join(output[0].split("_")[:-1])
        header = header + "*.tsv"
        try:
            shell("rm " + header)
        except:
            print("no file")
        print("removing " + header)
        
        output_dir = "/".join(output[0].split("/")[:-1])
        iterator = 0
        
        for filename in input:
            file_list = glob.glob("_".join(filename.split("_")[:-1]) + "_*.tsv")
            
            
            for file in file_list:
                rate = pd.read_csv(file, sep = "\t")
            
                rate.to_csv(output_dir + "/chunk_" + str(wildcards.chunksize) + "_" + str(iterator) + ".tsv", sep = "\t", index = None)
                iterator += 1
                  
rule get_length:
    input:
        os.path.join(scratch_dir, "kl_input/" +"{header}_chr_{chrom_all}/chunk_{chunksize}_0.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/"+"{header}_chr_{chrom_all}_chunk_{chunksize}_length.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb_length
    run:
        with Client() as client:
            
            input_filename = "_".join(input[0].split("_")[:-1]) + "_*.tsv"
            
            rate = dd.read_csv(input_filename, sep = "\t")
            
            length = len(rate)
            column_length = len(rate.columns)
            
            f = open(output[0], "w")
            
            f.write(str(length) + "\n")
            f.write(str(column_length) + "\n")
            
            f.close()
        
        
########################################## For running KL analysis #####################################
        
rule run_KL_cov:
    input:
        variants = os.path.join(scratch_dir, "kl_input/" + "{header}_chr_{chrom_all}/chunk_{chunksize}_0.tsv"),
        length_file = os.path.join(scratch_dir, "kl_input/" + "{header}_chr_{chrom_all}_chunk_{chunksize}_length.tsv"),
        neutral_sfs = os.path.join(KL_data_dir, "whole_genome/allele_freq/adaptive_bins/all.tsv"),
#         neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all.tsv",
#         neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all_original.tsv"
    output:
        os.path.join(KL_data_dir, "raklette_output/"+ "{header}_chr_{chrom_all}_chunksize_{chunksize}_covonly_lr_{learning_rate}_gamma_{gamma}_epoch_{epoch}_covprior_{cov_prior}.pkl")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=10,
        mem_mb=get_mem_mb_raklette
    run:
        
        input_filename = input.variants
        output_filename = output[0]
        neutral_sfs_filename = input.neutral_sfs
        
        input_length_file = input.length_file
        
        chunksize = int(wildcards.chunksize)
        num_epochs = int(wildcards.epoch)
        cov_prior = float(wildcards.cov_prior)
        learning_rate = float(wildcards.learning_rate)
        gamma = float(wildcards.gamma)
        
        input_directory = "/".join(input_filename.split("/")[:-1]) + "/"
    
        df = pd.read_csv(input_length_file, sep = "\t", header = None)
        nb_samples = df[0][0]
        nb_features = df[0][1] - 2

        print("number of samples: " + str(nb_samples), flush = True)

        if nb_samples == 0:
            f = open(output_filename, "w")
            f.write("no sample")
            f.close()
        else:        
            with open(input_filename) as f:
                first_line = f.readline()
            header = first_line.split("\t")

            print("number of chunks " + str(nb_samples/chunksize), flush = True)

            dataset = TSVDataset(input_directory, chunksize=chunksize, nb_samples = nb_samples, header_all = header, features = header)
            loader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False)

            #lets run raklette
            run_raklette_cov(loader, nb_features, num_epochs, neutral_sfs_filename, output_filename, 
                         float(learning_rate), float(gamma), 
                             cov_sigma_prior = torch.tensor(cov_prior, dtype=torch.float32))

