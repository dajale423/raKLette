## This is a snakemake to organize the dataframes for whole genomes and whole exomes
## I am making two different dataframes for the whole genomes and all of the exomes.

import sys
sys.path.insert(0,'/home/djl34/kl_git/scripts')
from snakefile_imports import *

n = 1

## function for allocating memory into 
def get_mem_mb(wildcards, attempt):
    return 10000 + (attempt + n) * 30000

chrom_set = all_chrom_set

def get_mem_mb_small(wildcards, attempt):
    return attempt * 6000 * factor

filename_list = [os.path.join(KL_data_dir, "whole_exome/wgs/{chrom}/_metadata")]
filename_list = [filename.format(chrom = chrom) for filename in filename_list for chrom in chrom_set]


rule all:
    input:
        os.path.join(KL_data_dir, "de_novo/zhou_etal/41588_2022_1148_MOESM5_ESM_hg38.tsv"),
        os.path.join(KL_data_dir, "de_novo/zhou_etal/41588_2022_1148_MOESM6_ESM_hg38.tsv"),
        os.path.join(KL_data_dir, "de_novo/zhou_etal/41588_2022_1148_MOESM7_ESM_hg38.tsv")



    
###################################################### mut model #######################################################
rule download_vova_model:
    input:
#         os.path.join(pd_data_dir, "vova_model/{chrom}_rate_v5.2_TFBS_correction.gz")
    output:
        os.path.join(pd_data_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz"),
        os.path.join(pd_data_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz.csi"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=500
    run:
        output_filename_0 = output[0].split("/")[-1]
        output_filename_1 = output[1].split("/")[-1]
        
        shell("wget -P /home/djl34/lab_pd/data/downloads/ http://genetics.bwh.harvard.edu/downloads/Vova/Roulette/{output_filename_0}")
        shell("wget -P /home/djl34/lab_pd/data/downloads/ http://genetics.bwh.harvard.edu/downloads/Vova/Roulette/{output_filename_1}")

        
##########################################################################################################################
#
#
#
#
#
############################################### for exonic regions #######################################################
# rule make_exonic_regions_file:
#     input:
#         os.path.join(pd_data_dir, "biomart/ENSG_ENST_ENSE_start_end_108.tsv")
#     output:
#         os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_exonic_regions_split.tsv")
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=1,
#         mem_mb=1000     
#     run:
#         df = pd.read_csv(input[0], sep = "\t")
#         df["Chromosome/scaffold name"] = df["Chromosome/scaffold name"].astype(str)
#         df = df[df["Chromosome/scaffold name"] == wildcards.chrom]
        
#         df = df.sort_values("Exon region start (bp)")
        
#         df = df[["Chromosome/scaffold name", "Exon region start (bp)", "Exon region end (bp)"]]
        
#         df["Exon region start (bp)"] = df["Exon region start (bp)"] - 5
#         df["Exon region end (bp)"] = df["Exon region end (bp)"] + 5
        
#         df.to_csv(output[0], sep = "\t", index = None)
            
        
# rule filter_for_exonic_regions:
#     input:
#         vcf = os.path.join(pd_data_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz"),
#         regions_file = os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_exonic_regions_split.tsv")
#     output:
#         os.path.join(scratch_dir, "whole_exome/{chrom}_rate_v5.2_TFBS_correction_whole_exon.vcf"),
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=1,
#         mem_mb=3000
#     shell:        
#         "bcftools view -R {input.regions_file} -o {output} {input.vcf}"
        
# rule run_vep:
#     input:
#         os.path.join(scratch_dir, "whole_exome/{chrom}_rate_v5.2_TFBS_correction_whole_exon.vcf"),
#     output:
#         os.path.join(scratch_dir, "whole_exome/{chrom}_rate_v5.2_TFBS_correction_whole_exon.txt")
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=16,
#         mem_mb=32000
#     shell:
#         """
#         module load gcc/6.2.0
#         module load perl/5.30.0
#         eval `perl -Mlocal::lib=~/perl5-O2`
#         {vep} --cache --offline -i {input} -o {output} --fork 16 --vcf --canonical --force_overwrite --no_stats --buffer_size 10000 --fasta /home/djl34/lab_pd/data/fasta/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna
#         """
# rule run_split_vep_for_mu:
#     input:
#         os.path.join(scratch_dir, "whole_exome/{chrom}_rate_v5.2_TFBS_correction_whole_exon.vcf")
#     output:
#         os.path.join(KL_data_dir, "whole_exome/vova_model/{chrom}_rate_v5.2_TFBS_correction_whole_exon.tsv")
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=1,
#         mem_mb=1000
#     shell:
#         """
#         bcftools query -f '%CHROM\t%POS\t%REF\t%ALT\t%FILTER\t%MR\t%AR\n' {input} > {output} 
#         """

        
# rule run_split_vep:
#     input:
#         os.path.join(scratch_dir, "whole_exome/{chrom}_rate_v5.2_TFBS_correction_whole_exon.txt")
#     output:
#         os.path.join(KL_data_dir, "whole_exome/vep/{chrom}_rate_v5.2_TFBS_correction_whole_exon.tsv")
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=1,
#         mem_mb=800
#     shell:
#         """
#         bcftools +split-vep {input} -f '%CHROM\t%POS\t%REF\t%ALT\t%Consequence\t%Gene\t%Feature\t%CANONICAL\n' -d > {output}
#         """
        

# rule get_mutation_rate:
#     input:
#         rate = os.path.join(KL_data_dir, "whole_exome/vep/{chrom}_rate_v5.2_TFBS_correction_whole_exon.tsv")
#     output:
#         os.path.join(scratch_dir, "whole_exome/mu_added/{chrom}.tsv"),
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=1,
#         mem_mb=3000
#     run:
#         with Client() as client:
#             names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'mu_quality', 'mu', 'mu_TFBS']

#             rate = dd.read_csv(input.rate, sep = "\t", names = names_list)
            
#             rate["mu"] = rate["mu_TFBS"].where(rate["mu_TFBS"] != ".", rate["mu"])
            
#             print(rate["mu_quality"].unique().compute())
            
#             rate = rate.drop(['mu_TFBS'], axis=1)
            
#             rate["mu"] = rate["mu"].astype(float)
            
# #             rate = rate.set_index(['Pos', 'Allele_ref', 'Allele'])
            
#             rate.to_csv(output[0], sep = "\t", index = None, single_file = True)

# rule add_vep:
#     input:
#         rate = os.path.join(scratch_dir, "whole_exome/mu_added/{chrom}.tsv"),
#         vep = os.path.join(KL_data_dir, "whole_exome/vep/{chrom}_rate_v5.2_TFBS_correction_whole_exon.tsv"),
#     output:
#         os.path.join(scratch_dir, "whole_exome/vep_added/{chrom}.tsv"),
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         with Client() as client:
            
#             rate = dd.read_csv(input.rate, sep = "\t")
#             names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'Consequence', 'Gene', 'Transcript', 'Canonical']

#             vep = dd.read_csv(input.vep, sep = "\t", names = names_list)
#             vep = vep[vep["Canonical"] == "YES"]
# #             vep = vep.set_index(['Pos', 'Allele_ref', 'Allele'])
            
#             #filter for high quality mutation rates
#             rate = rate[rate["mu_quality"].isin(["TFBS", "high"])]
            
#             rate = rate.merge(vep[["Pos", "Allele_ref", "Allele", "Consequence", "Gene", "Transcript", "Canonical"]], on = ["Pos", "Allele_ref", "Allele"], how = "left")
            
#             rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
        
# rule bin_mutation_rate:
#     input:
#         rate = os.path.join(scratch_dir, "whole_exome/vep_added/{chrom}.tsv"),
#         index = os.path.join(KL_data_dir, "whole_genome/mu_index.tsv")
#     output:
#         os.path.join(scratch_dir, "whole_exome/new_rate_bin/{chrom}/_metadata"),
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         with Client() as client:
            
#             rate = dd.read_csv(input.rate, sep = "\t")
            
#             names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'Consequence', 'Gene', 'Transcript', 'Canonical']
            
#             # first, bin some mutation rates
#             def bin_mutation_rate(mu):
#                 if mu < 0.4:
#                     if mu == 0.004:
#                         return 0.013
#                     if mu == 0.12:
#                         return 0.117
#                     if mu == 0.130:
#                         return 0.128
#                     if mu == 0.23:
#                         return 0.236
#                     if mu == 0.35:
#                         return 0.357
#                     return mu
#                 else:
#                     if mu < 0.6:
#                         return 0.5
#                     elif mu < 0.8:
#                         return 0.7
#                     elif mu < 1.2:
#                         return 1.0
#                     elif mu < 1.6:
#                         return 1.4
#                     elif mu > 3.219:
#                         return 3.219
#                     else:
#                         return mu
            
# #             rate["mu"].replace(".", None)
            
#             rate["mu"] = rate.apply(lambda row: bin_mutation_rate(row["mu"]), axis=1)
            
#             sfs = pd.read_csv(input.index, sep = "\t")
#             res = dict(zip(sfs["mu"], sfs.index))
            
#             def get_index(mu):
#                 try:
#                     return res[mu]
#                 except:
#                     return -1
            
#             rate["mu_index"] = rate.apply(lambda row: get_index(row["mu"]), axis=1)
            
            
#             rate["index"] = rate["Pos"].astype(str) + "_" + rate["Allele_ref"] + "_" + rate["Allele"] + "_" + rate["Gene"]
#             rate["index"] = rate["index"].astype(object)
            
#             rate = rate.dropna(subset=['index'])
            
#             rate = rate.set_index("index")
            
#             rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)
          


################################################ add_wgs_infromation
rule add_wgs:
    input:
        rate = os.path.join(KL_data_dir, "whole_exome/vep/{chrom}_rate_v5.2_TFBS_correction_whole_exon.tsv"),
        wgs = os.path.join(KL_data_dir, "whole_genome/freq_bins/{chrom}/_metadata"),
    output:
        os.path.join(KL_data_dir, "whole_exome/wgs/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'Consequence', 'Gene', 'Transcript', 'Canonical']

            vep = dd.read_csv(input.rate, sep = "\t", names = names_list, dtype={'Pos': 'Int64'})
            vep = vep[vep["Canonical"] == "YES"]
            vep = vep.drop(columns = ["Chrom"])

            wgs = dd.read_parquet(input.wgs.split("_metadata")[0])
            vep = vep.merge(wgs, on = ["Pos", "Allele_ref", "Allele"], how = "left")

            vep = vep.repartition(partition_size="3GB")
            
            vep.to_parquet(output[0].split("_metadata")[0], write_index = False, compression = "gzip", write_metadata_file = True)


################################################ add gnomADv4 values ################################################
rule run_split_vep_gnomAD_v4:
    input:
        os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.sites.chr{chrom}.vcf.bgz")
    output:
        os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.sites.chr{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=1000
    shell:
        """
        bcftools query -f '%CHROM\t%POS\t%REF\t%ALT\t%FILTER\t%AC\t%AN\t%AF\n' {input}  > {output}
        """
        
# rule run_split_vep_gnomAD_v4_vep:
#     input:
#         os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.sites.chr{chrom}.vcf.bgz")
#     output:
#         os.path.join(scratch_dir, "whole_exome/gnomad/gnomad.exomes.v4.0.sites.chr{chrom}_vep.tsv")
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=1,
#         mem_mb=1000
#     shell:
#         """
#         bcftools query -f '%CHROM\t%POS\t%REF\t%ALT\t%vep\n' {input}  > {output}
#         """
        
# rule split_by_variants:
#     input:
#         os.path.join(scratch_dir, "whole_exome/gnomad/gnomad.exomes.v4.0.sites.chr{chrom}_vep.tsv")
#     output:
#         os.path.join(scratch_dir, "whole_exome/gnomad/gnomad.exomes.v4.0.sites.chr{chrom}_split.tsv")
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=5,
#         mem_mb=15000
#     run:
#         from dask.distributed import Client

#         with Client() as client:
            
#             names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'vep']
#             ddf = dd.read_csv(input.add, sep = "\t", names = names_list)
            
#             ddf = ddf.assign(var1=df['vep'].str.split(',')).explode('vep')
            
#             ddf.to_csv(output[0], sep = "\t", index = None)

        
        
rule merge_gnomAD_v4:
    input:
        rate = os.path.join(scratch_dir, "whole_exome/new_rate_bin/{chrom}/_metadata"),
        add = os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.sites.chr{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "whole_exome/gnomADv4/{chrom}/_metadata")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=30000
    run:
        from dask.distributed import Client

        with Client() as client:
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
                        
            names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'filter_gnomADv4', 'AC_gnomADv4', 'AN_gnomADv4', 'AF_gnomADv4']
            ddf = dd.read_csv(input.add, sep = "\t", names = names_list, dtype={'AF_gnomADv4': 'object'})
#             ddf = ddf.set_index(['Pos', 'Allele_ref', 'Allele'])

            ddf['AF_gnomADv4'] = ddf['AF_gnomADv4'].replace(".", 0)
            ddf['AF_gnomADv4'] = ddf['AF_gnomADv4'].fillna(0)
            ddf['AF_gnomADv4'] = ddf['AF_gnomADv4'].astype(float)
            
            rate = rate.merge(ddf[["Pos", "Allele_ref", "Allele", "filter_gnomADv4", "AC_gnomADv4", "AN_gnomADv4", 'AF_gnomADv4']], on = ["Pos", "Allele_ref", "Allele"], how = "left")
            
            rate["Pos"] = dd.to_numeric(rate['Pos'], errors='coerce').fillna(0).astype(int)
            
            #get minor allele freq
            rate['AF_gnomADv4'] = rate['AF_gnomADv4'].fillna(0)
            rate["AF"] = rate["AF_gnomADv4"]
            rate["1-AF"] = 1 - rate["AF"]
            rate["MAF"] = rate[["AF", "1-AF"]].min(axis=1)
            
            rate = rate.drop(['AF', '1-AF', 'Canonical'], axis=1)
            
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

##FILTER=<ID=AC0,Description="Allele count is zero after filtering out low-confidence genotypes (GQ < 20; DP < 10; and AB < 0.2 for het calls)">
##FILTER=<ID=AS_VQSR,Description="Failed VQSR filtering thresholds of -1.4526 for SNPs and 0.0717 for indels">
##FILTER=<ID=InbreedingCoeff,Description="Inbreeding coefficient < -0.3">
##FILTER=<ID=PASS,Description="Passed all variant filters">
            
            
## add coverage
rule unzip_gnomAD_coverage:    
    input:
        os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.coverage.summary.tsv.bgz")
    output:
        os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.coverage.summary.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=1000
    shell:
        "gunzip -c {input} > {output}"
        
rule split_gnomAD_coverage:
    input:
        cov = os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.coverage.summary.tsv")
    output:
        os.path.join(scratch_dir, "downloads/chr{chrom}.gnomad.exomes.v4.0.coverage.summary.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            cov = dd.read_csv(input.cov, sep = "\t")
            cov["Chrom"] = cov["locus"].str.split(":", expand = True, n = 1)[0]            
            cov["Pos"] = cov["locus"].str.split(":", expand = True, n = 1)[1].astype(int)
            
            cov = cov[cov["Chrom"] == "chr" + wildcards.chrom]     
            
            cov["Pos"] = dd.to_numeric(cov['Pos'], errors='coerce').fillna(0).astype(int)
            cov = cov[cov["Pos"].isna() == False]
            
            cov.to_csv(output[0], sep = "\t", index = False, single_file = True)
            
rule add_gnomAD_coverage:
    input:
        rate = os.path.join(scratch_dir, "whole_exome/gnomADv4/{chrom}/_metadata"),
        cov = os.path.join(scratch_dir, "downloads/chr{chrom}.gnomad.exomes.v4.0.coverage.summary.tsv")
    output:
        os.path.join(scratch_dir, "whole_exome/gnomad_cov/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            ##'tsv' file is actually csv file
            cov = pd.read_csv(input.cov, sep = "\t")
            
            cov = cov.rename({"mean": "cov_mean_gnomADv4", "median_approx": "cov_median_gnomADv4"}, axis = 1)
            
            rate = rate.merge(cov[["Pos", "cov_mean_gnomADv4"]], on = ["Pos"], how = "left")
            
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
rule sort_by_pos:
    input:
        rate = os.path.join(scratch_dir, "whole_exome/gnomad_cov/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_exome/pos_sorted/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
#             rate["index"] = rate["Pos"].astype(str) + "_" + rate["Allele_ref"] + "_" + rate["Allele"] + "_" + rate["Gene"]
#             rate["index"] = rate["index"].astype(object)
            
#             rate = rate.dropna(subset=['index'])
            
#             rate = rate.set_index("index")
            
            #forward and backfille for AN
            rate["AN_gnomADv4_ffill"] = rate["AN_gnomADv4"].fillna(method = "ffill")
            rate["AN_gnomADv4_bfill"] = rate["AN_gnomADv4"].fillna(method = "bfill")
            rate['AN_gnomADv4_interpolate'] = rate[['AN_gnomADv4_bfill', 'AN_gnomADv4_ffill']].mean(axis=1)
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

            
rule filter_after_adding_gnomAD:
    input:
        rate = os.path.join(scratch_dir, "whole_exome/pos_sorted/{chrom}/_metadata"),
    output:
        os.path.join(KL_data_dir, "whole_exome/gnomad_filtered/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-0:20",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            rate = rate[(rate["filter_gnomADv4"].isna()) | ((rate["filter_gnomADv4"].str.contains("AS_VQSR") == False) & (rate["filter_gnomADv4"].str.contains("InbreedingCoeff") == False))]
            
            rate = rate[(rate["cov_mean_gnomADv4"] > 20) | (rate["cov_mean_gnomADv4"].isna())]
            
            
            #some allele numbers are set as 0.0, so let's filter them out just in case
            rate = rate[(rate["AN_gnomADv4_interpolate"] > 500000)]
            
            
            rate = rate.drop(['AN_gnomADv4_ffill', 'AN_gnomADv4_bfill'], axis=1)
            
            
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
        
####################################################### add region #######################################################
# mark which regions are neutral
rule filter_by_neutral_region:
    input:
        os.path.join(KL_data_dir, "whole_exome/gnomad_filtered/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_exome/neutral/{chrom}/_metadata"),
#         os.path.join(scratch_dir, "whole_exome/neutral_only/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            rate["neutral"] = 1
            
            rate["neutral"] = rate["neutral"].where(rate["Consequence"] == "synonymous_variant" , 0)

            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
#             rate_neutral = rate[rate["neutral"] == 1]
            
#             rate.to_parquet("/".join(output[1].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
####################################################### make AF bins #######################################################

# freq_breaks_9 = [-1, 1e-8, 2e-06, 4.0e-06, 8.0e-06, 1.6e-05, 5e-05, 5e-04, 5e-03, 0.5] 
# freq_breaks_10 = [-1, 1e-8, 1e-06, 1.5e-06, 3.0e-06, 1e-05, 5e-05, 5e-04, 5e-03, 5e-02, 0.5] 

# freq_breaks_9 = [-1, 1e-8, 1e-05, 1.7e-05, 2.3e-05, 3.6e-05, 8e-05, 5e-04, 5e-03, 0.5] 


rule make_adaptive_bins:
    input:
        os.path.join(scratch_dir, "whole_exome/neutral/{chrom}/_metadata"),
    output:
        os.path.join(KL_data_dir, "whole_exome/freq_bins/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=5000
    run:
        with Client() as client:
            
            freq_breaks_9 = [-1, 1e-8, 2e-06, 4.0e-06, 8.0e-06, 1.6e-05, 5e-05, 5e-04, 5e-03, 0.5] 
            freq_breaks_10 = [-1, 1e-8, 2e-06, 4.0e-06, 8.0e-06, 1.6e-05, 5e-05, 5e-04, 5e-03, 5e-02, 0.5] 
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            rate["Freq_bin_9"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_9, labels = False)
            rate["Freq_bin_10"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_10, labels = False)
            
            rate = rate.repartition(partition_size="3GB")

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
            
rule make_SFS_neutral:
    input:
        os.path.join(KL_data_dir, "whole_exome/freq_bins/{chrom}/_metadata"),
    output:
#         os.path.join(scratch_dir, "whole_exome/freq_bins/freq_bin_10_{chrom}.tsv"),
        os.path.join(scratch_dir, "whole_exome/freq_bins/freq_bin_9_{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=4000
    run:
        with Client() as client:
                        
            bin_list = ["Freq_bin_9"]
            
            i = 0
            for x in bin_list:
                
                df = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
                df = df[["mu", x, "neutral"]]
                df = df[df["neutral"] == 1]
                df = df.compute()

                df_group = pd.DataFrame(df.groupby(["mu", x])[x].count())

                df_group = df_group.rename({x: "count"}, axis = 1)

                df_group = df_group.reset_index()

                df_group["count"] = df_group["count"].astype(int)

                df_group_pivot = df_group.pivot(index='mu', columns=x, values='count')

                df_group_pivot = df_group_pivot.reset_index()

                df_group_pivot.to_csv(output[i], sep = "\t", index = None)
                i += 1
                            
rule combine_9_SFS_neutral:
    input:
        files = [os.path.join(scratch_dir, "whole_exome/freq_bins/freq_bin_9_" + chrom + ".tsv") for chrom in all_chrom_set]
    output:
        os.path.join(KL_data_dir, "whole_exome/freq_bins/freq_bin_9_all.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            df = dd.read_csv(input.files, sep = "\t", dtype={'0.0': 'float64',
                           '1.0': 'float64', '2.0': 'float64', '3.0': 'float64',
                           '4.0': 'float64', '5.0': 'float64', '6.0': 'float64',
                           '7.0': 'float64', '8.0': 'float64'})
    
            df = df.compute()
            
            df_group = df.groupby("mu")['0.0', '1.0', '2.0','3.0','4.0','5.0','6.0','7.0','8.0'].sum().reset_index()
            
            df_group = df_group.set_index('mu')
            sum_list = df_group.sum(axis = 1)
            df_group = df_group.div(df_group.sum(axis=1), axis=0)
            df_group["sum"] = sum_list
            df_group = df_group.reset_index()
            
            df_group.to_csv(output[0], sep = "\t", index = None)
        
# # rebin some mu together
# rule bin_mutation_rate_fix:
#     input:
#         rate = os.path.join(KL_data_dir, "whole_exome/freq_bins/{chrom}/_metadata"),
#         index = os.path.join(KL_data_dir, "whole_genome/mu_index.tsv")
#     output:
#         os.path.join(scratch_dir, "whole_exome/fix_rate_bin/{chrom}/_metadata"),
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         with Client() as client:
#             rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            
#             # first, bin some mutation rates
#             def bin_mutation_rate(mu):
#                 if mu > 3.219:
#                     return 3.219
#                 else:
#                     return mu
            
# #             rate["mu"].replace(".", None)
            
#             rate["mu"] = rate["mu"].astype(float)
#             rate["mu"] = rate.apply(lambda row: bin_mutation_rate(row["mu"]), axis=1)
                        
#             rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
            
################################################ to run nonsense_loftee ################################################
rule make_nonsense_regions_file:
    input:
        os.path.join(KL_data_dir, "whole_exome/vep/{chrom}_rate_v5.2_TFBS_correction_whole_exon.tsv")
    output:
        os.path.join(scratch_dir, "whole_exome/nonsense/{chrom}_whole_exon_nonsense_regions_file.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=4000     
    run:
        with Client() as client:

            names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'Consequence', 'Gene', 'Transcript', 'Canonical']

            ddf = dd.read_csv(input[0], sep = "\t", names = names_list)
            
            df_nonsense = ddf[(ddf["Consequence"].str.contains("splice_acceptor_variant")) | (ddf["Consequence"].str.contains("splice_donor_variant")) | (ddf["Consequence"].str.contains("stop_gained"))].compute()
            
            df_nonsense[["Chrom", "Pos"]].to_csv(output[0], sep = "\t", index = None, header = False)
        
rule filter_for_nonsense_sites:
    input:
        vcf = os.path.join(pd_data_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz"),
        regions_file = os.path.join(scratch_dir, "whole_exome/nonsense/{chrom}_whole_exon_nonsense_regions_file.tsv")
    output:
        os.path.join(scratch_dir, "whole_exome/nonsense/{chrom}_rate_v5.2_TFBS_correction_whole_nonsense.vcf"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=2000
    shell:        
        "bcftools view -R {input.regions_file} -o {output} {input.vcf}"
        
rule run_vep_loftee:
    input:
        os.path.join(scratch_dir, "whole_exome/nonsense/{chrom}_rate_v5.2_TFBS_correction_whole_nonsense.vcf"),
    output:
        os.path.join(scratch_dir, "whole_exome/nonsense/{chrom}_rate_v5.2_TFBS_correction_whole_nonsense_loftee_38.vcf")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=8,
        mem_mb=12000
    shell:
        """
        module load gcc/6.2.0
        module load perl/5.30.0
        eval `perl -Mlocal::lib=~/perl5-O2`
        export PERL5LIB=$PERL5LIB:/home/djl34/lab_pd/data/for_loftee/:$HOME/cpanm/lib/perl5
        {vep} --cache --offline -i {input} -o {output} --fork 8 --vcf --canonical --force_overwrite --no_stats --buffer_size 10000 --fasta /home/djl34/lab_pd/data/fasta/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --plugin LoF,loftee_path:/home/djl34/lab_pd/git/loftee,human_ancestor_fa:/home/djl34/lab_pd/data/human_ancestor/hg38/human_ancestor.fa.gz,conservation_file:/home/djl34/lab_pd/data/for_loftee/loftee.sql,gerp_bigwig:/home/djl34/lab_pd/data/for_loftee/gerp_conservation_scores.homo_sapiens.GRCh38.bw --dir_plugins /home/djl34/lab_pd/git/loftee    
        """
        
# rule run_vep_loftee_38:
#     input:
#         os.path.join(scratch_dir, "whole_exome/nonsense/{chrom}_rate_v5.2_TFBS_correction_whole_nonsense.vcf"),
#     output:
#         os.path.join(scratch_dir, "whole_exome/nonsense/{chrom}_rate_v5.2_TFBS_correction_whole_nonsense_loftee_38.vcf")
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=8,
#         mem_mb=14000
#     shell:
#         """
#         module load gcc/6.2.0
#         module load perl/5.30.0
#         export PERL5LIB=$PERL5LIB:/home/djl34/lab_pd/data/for_loftee/
#         eval `perl -Mlocal::lib=~/perl5-O2`
#         {vep} --cache --offline -i {input} -o {output} --fork 8 --vcf --canonical --force_overwrite --no_stats --buffer_size 10000 --fasta /home/djl34/lab_pd/data/fasta/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --plugin LoF,loftee_path:/home/djl34/lab_pd/git/loftee,human_ancestor_fa:/home/djl34/lab_pd/data/human_ancestor/hg38/human_ancestor.fa.gz,conservation_file:/home/djl34/lab_pd/data/for_loftee/loftee.sql,gerp_bigwig:/home/djl34/lab_pd/data/for_loftee/gerp_conservation_scores.homo_sapiens.GRCh38.bw --dir_plugins /home/djl34/lab_pd/git/loftee_38        
#         """
        
rule run_split_vep_loftee:
    input:
        os.path.join(scratch_dir, "whole_exome/nonsense/{chrom}_rate_v5.2_TFBS_correction_whole_nonsense_loftee_38.vcf")
    output:
        os.path.join(KL_data_dir, "whole_exome/nonsense/{chrom}_loftee_38.tsv")
    resources:
        partition="short",
        runtime="0-01:00",
        cpus_per_task=1,
        mem_mb=1000
    shell:
        """
        bcftools +split-vep {input} -f '%CHROM\t%POS\t%REF\t%ALT\t%Consequence\t%Gene\t%Feature\t%CANONICAL\t%LoF\t%LoF_filter\t%LoF_flags\t%LoF_info\n' -d > {output}
        """

rule filter_nonsense_from_freq_bins:
    input:
        os.path.join(KL_data_dir, "whole_exome/freq_bins/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_exome/freq_bins/{chrom}_lof.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=5000
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            rate_nonsense = rate[(rate["Consequence"].str.contains("splice_acceptor_variant")) | (rate["Consequence"].str.contains("splice_donor_variant")) | (rate["Consequence"].str.contains("stop_gained"))].compute()
            
            rate_nonsense.to_csv(output[0], sep = "\t", index = None)      
        
# rebin some mu together
rule merge_loftee:
    input:
        rate = os.path.join(scratch_dir, "whole_exome/freq_bins/{chrom}_lof.tsv"),
        nonsense = os.path.join(KL_data_dir, "whole_exome/nonsense/{chrom}_loftee_38.tsv")
    output:
        os.path.join(KL_data_dir, "whole_exome/nonsense/HC_{chrom}.tsv"),
#         os.path.join(KL_data_dir, "whole_exome/nonsense/not_LC_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=get_mem_mb_small
    run:
        with Client() as client:
            rate = pd.read_csv(input[0], sep = "\t")
                        
            names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'Consequence', 'Gene', 'Transcript', 'Canonical', 'LoF', 'LoF_filter', 'LoF_flags', 'LoF_info']

            df = pd.read_csv(input.nonsense, sep = "\t", names = names_list)
            
            df = df[df["Canonical"] == "YES"]
            df = df[(df["Consequence"].str.contains("splice_acceptor_variant")) | (df["Consequence"].str.contains("splice_donor_variant")) | (df["Consequence"].str.contains("stop_gained"))]
            df = df[df["LoF"] == "HC"]
            
            df = df[["Pos", "Allele_ref", "Allele", "LoF", "LoF_filter", "LoF_flags", "LoF_info"]].merge(rate, on = ["Pos", "Allele", "Allele_ref"], how = "left")
            
            df = df[df["Chrom"].isna() == False]

            df = df.drop_duplicates(subset = ["Pos", "Allele", "Gene", "Transcript"])
            
            df.to_csv(output[0], sep = "\t", index = None)            
            
        
# rule merge_loftee:
#     input:
#         rate = [os.path.join(KL_data_dir, "whole_exome/nonsense/" + chrom + "_loftee.tsv") for chrom in all_chrom_set],
#         gnomad = os.path.join(KL_data_dir, "whole_exome/gnomad_filtered/*/_metadata"),
#     output:
#         os.path.join(KL_data_dir, "whole_exome/nonsense/HC_loftee_gnomad_filtered.tsv")
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         from dask.distributed import Client

#         with Client() as client:
#             names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'Filter', 'mu', 'Consequence', 'Gene', 'Transcript', "LoF", 'LoF_filter', 'LoF_flags', 'LoF_info']

#             ddf = dd.read_csv(input.rate, sep = "\t", names = names_list)
            
#             rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            
#             df_hq = df[df["mu_quality"].isin(["TFBS", "high"])]

#             df_hq["mu"] = df_hq["mu_TFBS"].where(df_hq["mu_TFBS"].isna() == False, df_hq["mu"])

#             df_hq = df_hq.drop(['mu_quality', 'mu_TFBS'], axis=1)

#             df_hq["Allele_ref"] = df_hq["Pentamer"].str[2]
#             df_hq["Allele"] = df_hq["Pentamer"].str[6]

#             df_hq = df_hq.drop(['Pentamer'], axis=1)
            
#             df_hq = df_hq.repartition(partition_size="2GB")
            
#             df_hq.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)  

################################################ work on de novo files ################################################

rule convert_to_hg38:
    input:
        os.path.join(KL_data_dir, "de_novo/zhou_etal/{header}.xlsx")
    output:
        os.path.join(KL_data_dir, "de_novo/zhou_etal/{header}_hg38.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=3000,
    run:
        df = pd.read_excel(input[0])
        df = df.rename({"Position": "Pos_hg19"}, axis = 1)

        df["Pos"] = df.apply(lambda row: genomic.get_hg38_pos(row["Chrom"], row["Pos_hg19"]), axis=1).astype('Int64')
        df = df[~df["Pos"].isna()]

        df.to_csv(output[0], sep = "\t", index = None)


