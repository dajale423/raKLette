## This is a snakemake to organize the dataframes for whole genomes
## I am making two different dataframes one will contain the features, the other will contain population sequencing data

import sys
sys.path.insert(0,'/home/djl34/kl_git/scripts')
from snakefile_imports import *

n = 0

## function for allocating memory into 
def get_mem_mb(wildcards, attempt):
    return 10000 + (attempt + n) * 30000

include: "/home/djl34/kl_git/workflow/preprocessing/download/Snakefile"
chrom_set = all_chrom_set
# chrom_set = ["22"]

##############################################################################################################################

    
## for AF
# filename_list = [os.path.join(scratch_dir, "whole_genome/ukb_ac/{chrom}/_metadata"), 
#                  os.path.join(KL_data_dir, "whole_genome/filtered/{chrom}/_metadata"),
#                  os.path.join(scratch_dir, "whole_genome/AN_interpolate/{chrom}/_metadata"),
#                 os.path.join(KL_data_dir, "whole_genome/neutral/{chrom}/_metadata")]
#                 # os.path.join(KL_data_dir, "whole_genome/filtered/{chrom}/_metadata"),
#                 # os.path.join(KL_data_dir, "whole_genome/cds/{chrom}/_metadata"),
#                 # os.path.join(KL_data_dir, "whole_genome/neutral/expected_AC/{chrom}.tsv")]

# filename_list = [os.path.join(scratch_dir, "whole_genome/AN_interpolate/{chrom}/_metadata"),
#                  os.path.join(scratch_dir, "whole_genome/filtered/{chrom}.tsv"),
#                  os.path.join(scratch_dir, "whole_genome/filtered_additional/{chrom}.tsv"),
#                  os.path.join(KL_data_dir, "whole_genome/filtered_additional/{chrom}.tsv"),
#                  os.path.join(KL_data_dir, "whole_genome/combined/footprints/{chrom}/_metadata"),
#                  os.path.join(KL_data_dir, "whole_genome/combined/add_gene_info/{chrom}/_metadata"),
#                  os.path.join(KL_data_dir, "whole_genome/combined/add_gene_info/{chrom}.tsv.gz")]

filename_list = [os.path.join(scratch_dir, "whole_genome/p_9/{chrom}/_metadata"),
                 os.path.join(KL_data_dir, "whole_genome/p/{chrom}/_metadata"),
                 os.path.join(KL_data_dir, "whole_genome/zoonomia_phastcons_primates/{chrom}/_metadata")]


## for epigenetic features
# filename_list = [os.path.join(scratch_dir, f"whole_genome/spliceai/{chrom}/_metadata") from chrom in chrom_set]
## for windows
# filename_list = [os.path.join(scratch_dir, "whole_genome/windows/{chrom}/_metadata")]

filename_list = [filename.format(chrom = chrom) for filename in filename_list for chrom in chrom_set]

# filename_list = [filename.replace("{chrom}", str(chromosome)) for filename in filename_list for chromosome in chrom_set]

rule all:
    input:
        filename_list,
        # os.path.join(KL_data_dir, "whole_genome/freq_bins/freq_bin_2_all.tsv"),
        # os.path.join(KL_data_dir, "whole_genome/freq_bins/freq_bin_9_all.tsv")

################################################### add allele count info ####################################################
        
# rule unzip_vova_model:
#     input:
#         os.path.join(pd_data_dir, "vova_model/{chrom}_rate_v5.2_TFBS_correction.gz")
#     output:
#         os.path.join(scratch_dir, "vova_model/{chrom}_rate_v5.2_TFBS_correction")
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=1,
#         mem_mb=get_mem_mb_small
#     shell:
#         "gunzip -c {input} > {output}"
                
rule filter_low_quality_files:
    input:
        os.path.join(scratch_dir, "vova_model/{chrom}_rate_v5.2_TFBS_correction")
    output:
        os.path.join(KL_data_dir, "whole_genome/mu_filtered/{chrom}/_metadata")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=20000
    run:
        from dask.distributed import Client

        with Client() as client:
            header = ["Pos", "Pentamer", "mu", "mu_quality", "mu_TFBS"]

            df = dd.read_csv(input[0], sep = " ", names = header)
            
            df_hq = df[df["mu_quality"].isin(["TFBS", "high"])]

            df_hq["mu"] = df_hq["mu_TFBS"].where(df_hq["mu_TFBS"].isna() == False, df_hq["mu"])

            df_hq = df_hq.drop(['mu_quality', 'mu_TFBS'], axis=1)

            df_hq["Allele_ref"] = df_hq["Pentamer"].str[2]
            df_hq["Allele"] = df_hq["Pentamer"].str[6]

            df_hq = df_hq.drop(['Pentamer'], axis=1)
            
            df_hq = df_hq.repartition(partition_size="2GB")
            
            df_hq.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
rule bin_mutation_rate:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/mu_filtered/{chrom}/_metadata"),
        index = os.path.join(KL_data_dir, "whole_genome/mu_index.tsv")
    output:
        os.path.join(scratch_dir, "whole_genome/new_rate_bin/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-10:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            # first, bin some mutation rates
            def bin_mutation_rate(mu):
                if mu < 0.4:
                    if mu == 0.004:
                        return 0.013
                    if mu == 0.12:
                        return 0.117
                    if mu == 0.130:
                        return 0.128
                    if mu == 0.23:
                        return 0.236
                    if mu == 0.35:
                        return 0.357
                    return mu
                else:
                    if mu < 0.6:
                        return 0.5
                    elif mu < 0.8:
                        return 0.7
                    elif mu < 1.2:
                        return 1.0
                    elif mu < 1.6:
                        return 1.4
                    elif mu > 3.5:
                        return 3.55
                    else:
                        return mu
            
#             rate["mu"].replace(".", None)
            
            rate["mu"] = rate.apply(lambda row: bin_mutation_rate(row["mu"]), axis=1)
            
            sfs = pd.read_csv(input.index, sep = "\t")
            res = dict(zip(sfs["mu"], sfs.index))
            
            def get_index(mu):
                try:
                    return res[mu]
                except:
                    return -1
            
            rate["mu_index"] = rate.apply(lambda row: get_index(row["mu"]), axis=1)
            
#             rate["index"] = rate["Pos"].astype(str) + "_" + rate["Allele_ref"] + "_" + rate["Allele"]
#             rate["index"] = rate["index"].astype(object)
            
#             rate = rate.dropna(subset=['index'])
            
#             rate = rate.set_index("index")

            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
            
            
################################################### add allele count info ####################################################
## add gnomAD info

## coverage information
            
# rule download_gnomad_v3_cov:
#     input:
#     output:
#         os.path.join(scratch_dir, "downloads/gnomad.genomes.r3.0.1.coverage.summary.tsv.bgz")
#     resources:
#         partition="short",
#         runtime="0-10:00",
#         cpus_per_task=1,
#         mem_mb=2000
#     shell:
#         "wget -P /n/scratch3/users/d/djl34/downloads/ https://storage.googleapis.com/gcp-public-data--gnomad/release/3.0.1/coverage/genomes/gnomad.genomes.r3.0.1.coverage.summary.tsv.bgz"

# rule unzip_gnomad_v3_cov:
#     input:
#         os.path.join(scratch_dir, "downloads/gnomad.genomes.r3.0.1.coverage.summary.tsv.bgz")
#     output:
#         os.path.join(scratch_dir, "downloads/gnomad.genomes.r3.0.1.coverage.summary.tsv")
#     resources:
#         partition="short",
#         runtime="0-10:00",
#         cpus_per_task=1,
#         mem_mb=2000
#     shell:
#         "gunzip -c {input} > {output}"

# rule add_gnomAD_v3_coverage:
#     input:
#         os.path.join(scratch_dir, "whole_genome/new_rate_bin/{chrom}/_metadata"),
#         os.path.join(pd_data_dir, "gnomadv3.1.2/cov/{chrom}/_metadata")
#     output:
#         os.path.join(scratch_dir, "whole_genome/gnomad_cov/{chrom}/_metadata"),
#     resources:
#         partition="short",
#         runtime="0-10:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         with Client() as client:
            
#             rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
# #             rate["AN_ukb_ffill"] = rate["AN_ukb_ffill"].astype('Int64')
# #             rate["AN_ukb_bfill"] = rate["AN_ukb_bfill"].astype('Int64')
# #             rate["AN_gnomADv3_bfill"] = rate["AN_gnomADv3_bfill"].astype('Int64')
# #             rate["AN_gnomADv3_ffill"] = rate["AN_gnomADv3_ffill"].astype('Int64')
            
#             gnomad = dd.read_parquet("/".join(input[1].split("/")[:-1]) + "/")
#             gnomad = gnomad.rename(columns={"mean": "cov_mean_gnomADv3", "median_approx": "cov_median_gnomADv3"})
# #             gnomad["Pos"] = gnomad["Pos"].astype('Int64')
            
#             rate = rate.merge(gnomad[["Pos", "cov_median_gnomADv3"]], on = ["Pos"], how = "left")
            
#             rate = rate[(rate["cov_median_gnomADv3"] > 25) & (rate["cov_median_gnomADv3"] < 35)] 
            
#             rate = rate.drop(columns=['cov_median_gnomADv3'])

#             rate = rate.repartition(partition_size="3GB")
            
#             rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)

# rule add_gnomAD_v3_AC:
#     input:
#         os.path.join(scratch_dir, "whole_genome/gnomad_cov/{chrom}/_metadata"),
#         os.path.join(pd_data_dir, "gnomadv3.1.2/AC/{chrom}/_metadata")
# #         os.path.join(pd_data_dir, "gnomadv3/AC/{chrom}/_metadata")
#     output:
#         os.path.join(KL_data_dir, "whole_genome/gnomad_ac/{chrom}/_metadata"),
#     resources:
#         partition="short",
#         runtime="0-10:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         from dask.distributed import Client

#         with Client() as client:
#             rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
#             gnomad = dd.read_parquet("/".join(input[1].split("/")[:-1]) + "/")
            
#             gnomad = gnomad.rename(columns={"Filter": "filter_gnomADv3", "AC": "AC_gnomADv3", "AN": "AN_gnomADv3"})
            
# #             gnomad["index"] = gnomad["Pos"].astype(str) + "_" + gnomad["Allele_ref"] + "_" + gnomad["Allele"]
# #             gnomad["index"] = gnomad["index"].astype(object)
# #             gnomad = gnomad.dropna(subset=['index'])
# #             gnomad = gnomad.set_index("index")
            
# #             include_filter = ["PASS", "AC0"]
            
#             rate = rate.merge(gnomad[["Pos", "Allele_ref", "Allele", "filter_gnomADv3", "AC_gnomADv3", "AN_gnomADv3"]], how = "left", on = ["Pos", "Allele_ref", "Allele"])
            
#             rate = rate.dropna(subset=['Pos', 'Allele_ref', 'Allele'])
            
#             rate["Pos"] = dd.to_numeric(rate['Pos'], errors='coerce').fillna(0).astype('Int64')
            
#             rate = rate.repartition(partition_size="3GB")

#             rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
rule add_ukb_AC:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/gnomad_ac/{chrom}/_metadata"),
        ukb = os.path.join(pd_data_dir, "ukbiobank/500k/split_chr{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "whole_genome/ukb_ac/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-10:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        from dask.distributed import Client

        with Client() as client:
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            ukb = dd.read_csv(input.ukb, sep = "\t", dtype={'filter': 'object', 'Pos': 'Int64', 'AC': "Int64", 'AN': 'Int64'}) 
            ukb = ukb.rename(columns = {"Filter": "filter_ukb", "AN": "AN_ukb", "AC": "AC_ukb"})
            
            rate = rate.merge(ukb[["Pos", "Allele_ref", "Allele", "filter_ukb", "AC_ukb", "AN_ukb"]], on = ["Pos", "Allele_ref", "Allele"], how = "left")
            
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
# rule set_index:
#     input:
#         os.path.join(scratch_dir, "whole_genome/ukb_ac/{chrom}/_metadata"),
#     output:
#         os.path.join(scratch_dir, "whole_genome/set_index/{chrom}/_metadata"),
#     resources:
#         partition="short",
#         runtime="0-10:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         with Client() as client:
#             rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
#             rate["index"] = rate["Pos"].astype(str) + "_" + rate["Allele_ref"] + "_" + rate["Allele"]
#             rate["index"] = rate["index"].astype(object)

#             rate = rate.dropna(subset=['index'])

#             rate = rate.set_index("index")
            
#             rate = rate.repartition(partition_size="3GB")

#             rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)
            
rule sort_values_by_pos:
    input:
        os.path.join(scratch_dir, "whole_genome/ukb_ac/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/sort_by_pos/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-10:00",
        cpus_per_task=5,
        mem_mb=120000
    run:
        with Client() as client:
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            rate = rate.sort_values('Pos')  
            
#             rate = rate.repartition(partition_size="3GB")

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)

rule interpolate_AN:
    input:
        rate = os.path.join(scratch_dir, "whole_genome/sort_by_pos/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/AN_interpolate/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            #forward and backfille for AN
            rate["AN_gnomADv3_ffill"] = rate["AN_gnomADv3"].fillna(method = "ffill").astype('Int64')
            rate["AN_gnomADv3_bfill"] = rate["AN_gnomADv3"].fillna(method = "bfill").astype('Int64')
            rate['AN_gnomADv3_interpolate'] = rate[['AN_gnomADv3_bfill', 'AN_gnomADv3_ffill']].mean(axis=1)
            
            #forward and backfille for AN UKB
            rate["AN_ukb_ffill"] = rate["AN_ukb"].fillna(method = "ffill").astype('Int64')
            rate["AN_ukb_bfill"] = rate["AN_ukb"].fillna(method = "bfill").astype('Int64')
            rate['AN_ukb_interpolate'] = rate[['AN_ukb_bfill', 'AN_ukb_ffill']].mean(axis=1)

            #forward and backfille for AN filter
            rate["filter_ukb_ffill"] = rate["filter_ukb"].fillna(method = "ffill")
            rate["filter_ukb_bfill"] = rate["filter_ukb"].fillna(method = "bfill")

            rate["filter_gnomADv3_ffill"] = rate["filter_gnomADv3"].fillna(method = "ffill")
            rate["filter_gnomADv3_bfill"] = rate["filter_gnomADv3"].fillna(method = "bfill")
            
            
            rate["AN_total_interpolate"] = rate['AN_gnomADv3_interpolate'] + rate['AN_ukb_interpolate']
            rate["AC_total"] = rate["AC_ukb"].fillna(0) + rate["AC_gnomADv3"].fillna(0)
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)
            
rule filter_sites:
    input:
        os.path.join(scratch_dir, "whole_genome/AN_interpolate/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/filtered/{chrom}.tsv"),
        # os.path.join(KL_data_dir, "whole_genome/filtered/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            rate = rate.drop(columns=['AN_gnomADv3_ffill', 'AN_gnomADv3_bfill', 'AN_gnomADv3_interpolate', 'AN_ukb_interpolate', 'AN_ukb_ffill', 'AN_ukb_bfill'])

            include_filter = ["PASS", "AC0"]
            for filter_interpolate in ["filter_gnomADv3_ffill", "filter_gnomADv3_bfill"]:
                rate[filter_interpolate + "_int"] = rate[filter_interpolate].isin(include_filter).astype(int)
            
            include_filter = ["PASS"]       
            for filter_interpolate in ["filter_ukb_ffill", "filter_ukb_bfill"]:
                rate[filter_interpolate + "_int"] = rate[filter_interpolate].isin(include_filter).astype(int)

            rate["filter_int"] = (rate["filter_gnomADv3_ffill_int"] + rate["filter_gnomADv3_bfill_int"] + 
                                  rate["filter_ukb_ffill_int"] + rate["filter_ukb_bfill_int"])


            rate = rate[rate["filter_int"] >= 3]

            rate = rate.drop(columns=['filter_gnomADv3_ffill', 'filter_gnomADv3_bfill', 'filter_ukb_ffill', 'filter_ukb_bfill',
                                      'filter_gnomADv3_ffill_int', 'filter_gnomADv3_bfill_int', 'filter_ukb_ffill_int', 'filter_ukb_bfill_int'])
            
            rate["AF_gnomADv3"] = rate["AC_gnomADv3"]/rate["AN_gnomADv3"]
            
            rate["AF_ukb"] = rate["AC_ukb"]/rate["AN_ukb"]
            
            rate["AF"] = rate["AC_total"]/rate["AN_total_interpolate"]
            
            rate = rate.drop(columns=['filter_ukb', 'filter_gnomADv3', 'AN_gnomADv3', 'AN_ukb', 'AC_ukb', 'AC_gnomADv3'])
            
            rate = rate[rate["AF"] <= 0.5]

            rate = rate[rate["AN_total_interpolate"] > 1000000]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)


            
            # rate = rate.repartition(partition_size="3GB")
            # rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)

# check for number of mutations in 100bp window
rule additional_filtering_100bp_window:
    input:
        os.path.join(scratch_dir, "whole_genome/filtered/{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "whole_genome/filtered_additional/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            # rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            rate = dd.read_csv(input[0], sep = "\t")

            min_pos = rate["Pos"].min().compute()
            rate["window_100bp"] = ((rate["Pos"] - min_pos)/100).astype(int)
            rate["polymorphic"] = rate["AC_total"].astype(bool).astype(int)

            rate_polymorphic_sum = rate[["window_100bp", "polymorphic"]].groupby("window_100bp").sum().reset_index()
            rate = rate.merge(rate_polymorphic_sum, on = "window_100bp", how = 'left')
            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            #polymorphic_x refers to polymorphic status, polymorphic_y refers to number of polymorphic sites in 100bp window
            
            # rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)

rule save_100bp_window:
    input:
        os.path.join(scratch_dir, "whole_genome/filtered_additional/{chrom}.tsv"),
    output:
        os.path.join(KL_data_dir, "whole_genome/filtered_additional/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            # rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            rate = dd.read_csv(input[0], sep = "\t")

            df = rate[["window_100bp", "polymorphic_y"]].drop_duplicates().compute()
            df.to_csv(output[0], sep = "\t", index = None)


rule filter_out_100bp_window:
    input:
        os.path.join(scratch_dir, "whole_genome/filtered_additional/{chrom}.tsv"),
    output:
        os.path.join(KL_data_dir, "whole_genome/filtered/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_csv(input[0], sep = "\t")
            
            rate = rate[rate["polymorphic_y"] > 5]
            rate = rate.rename(columns = {"polymorphic_x": "polymorphic", "polymorphic_y": "polymorphic_100bp"})

            rate = rate.repartition(partition_size="3GB")

            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)

##################################################### add neutral region ####################################################
# mark which regions are neutral
rule filter_by_neutral_region:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/filtered/{chrom}/_metadata"),
        neutral = pd_data_dir + "/NRE/NRE_result_all.tsv"
    output:
        os.path.join(KL_data_dir, "whole_genome/neutral/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=3,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            rate["Pos"] = rate["Pos"].astype('Int64')
            
            df = pd.read_csv(input.neutral, sep = "\t")
            df = df[df["chrom"] == "chr" + str(wildcards.chrom)]
            df = df[["chromStart", "chromEnd"]]
            
            def get_range(start, end):
                return range(start, end + 1)
            
            df["Pos_hg19"] = df.apply(lambda row: get_range(row["chromStart"],row["chromEnd"]), axis=1)
            df = df.explode('Pos_hg19')
            df["Pos"] = df.apply(lambda row: genomic.get_hg38_pos(str(wildcards.chrom), row["Pos_hg19"]), axis=1)
            
            df = df[df["Pos"].isna() == False]
            df["Pos"] = df["Pos"].astype('Int64')
            
            df["Neutral"] = 1
            
            rate.reset_index()
            
            rate = rate.merge(df[["Pos", "Neutral"]], on = "Pos", how = "left")
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
            
# mark which regions are exonic
rule add_cds_regions:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/neutral/{chrom}/_metadata"),
        cds = pd_data_dir + "/biomart/ENSG_exon_start_end_type_111.tsv"
    output:
        os.path.join(KL_data_dir, "whole_genome/cds/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=3,
        mem_mb=get_mem_mb
    run:
        with Client() as client:        
            biomart = pd.read_csv(input.cds, sep = "\t")
            
            biomart["Chromosome/scaffold name"] = biomart["Chromosome/scaffold name"].astype(str)
            biomart_chr = biomart[biomart["Chromosome/scaffold name"] == wildcards.chrom]
            biomart_chr = biomart_chr[biomart_chr["Gene type"] == "protein_coding"]
            biomart_chr = biomart_chr[["Exon region start (bp)", "Exon region end (bp)"]].drop_duplicates(keep = "first")

            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            def get_range(start, end):
                return range(start, end + 1)

            biomart_chr["Pos"] = biomart_chr.apply(lambda row: get_range(row["Exon region start (bp)"], row["Exon region end (bp)"]), axis=1)
            list_positions = list(set([a for b in biomart_chr.Pos.tolist() for a in b]))
            
            
            ##exon colum refers to the CDS for protein_coding genes
            rate["cds"] = rate["Pos"].where(rate["Pos"].isin(list_positions) == True, 0)
                                               
            rate["cds"] = rate["cds"].astype(bool)
            
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)
                        
####################################################### make AF bins #######################################################            
rule make_adaptive_bins:
    input:
        os.path.join(KL_data_dir, "whole_genome/cds/{chrom}/_metadata"),
    output:
        os.path.join(KL_data_dir, "whole_genome/freq_bins/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=3,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            filename = input[0]
            
            rate = dd.read_parquet("/".join(filename.split("/")[:-1]) + "/")
            
            rate["Freq_bin_9"] = rate['AF'].map_partitions(pd.cut, freq_breaks_9, labels = False)
            rate["Freq_bin_10"] = rate['AF'].map_partitions(pd.cut, freq_breaks_10, labels = False)
            
            rate = rate.repartition(partition_size="3GB")

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)
            
rule make_SFS_neutral:
    input:
        os.path.join(KL_data_dir, "whole_genome/freq_bins/{chrom}/_metadata"),
    output:
#         os.path.join(scratch_dir, "whole_exome/freq_bins/freq_bin_10_{chrom}.tsv"),
        os.path.join(scratch_dir, "whole_genome/freq_bins/freq_bin_9_{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=4000
    run:
        with Client() as client:
                        
            bin_list = ["Freq_bin_9"]
            
            i = 0
            for x in bin_list:
                
                df = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
                df = df[["mu", x, "Neutral"]]
                df = df[df["Neutral"] == 1]
                df = df.compute()

                df_group = pd.DataFrame(df.groupby(["mu", x])[x].count())

                df_group = df_group.rename({x: "count"}, axis = 1)

                df_group = df_group.reset_index()

                df_group["count"] = df_group["count"].astype(int)

                df_group_pivot = df_group.pivot(index='mu', columns=x, values='count')

                df_group_pivot = df_group_pivot.reset_index()

                df_group_pivot.to_csv(output[i], sep = "\t", index = None)
                i += 1

rule make_SFS_neutral_other_freq_bins:
    input:
        os.path.join(KL_data_dir, "whole_genome/freq_bins/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/freq_bins/freq_bin_2_{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=4000
    run:
        with Client() as client:
                        
            i = 0

            bin_list = ["Freq_bin_2"]
            
            for x in bin_list:
                
                df = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
                df = df[["mu", "Neutral", "AF"]]
                df = df[df["Neutral"] == 1]
                df[x] = df['AF'].map_partitions(pd.cut, freq_breaks_2, labels = False)

                df = df.compute()
                df_group = pd.DataFrame(df.groupby(["mu", x])[x].count())

                df_group = df_group.rename({x: "count"}, axis = 1)
                df_group = df_group.reset_index()

                df_group["count"] = df_group["count"].astype(int)
                df_group_pivot = df_group.pivot(index='mu', columns=x, values='count')
                df_group_pivot = df_group_pivot.reset_index()

                df_group_pivot.to_csv(output[i], sep = "\t", index = None)
                i += 1
                            
rule combine_9_SFS_neutral:
    input:
        files = [os.path.join(scratch_dir, "whole_genome/freq_bins/freq_bin_{freq_bin}_" + chrom + ".tsv") for chrom in all_chrom_set]
    output:
        os.path.join(KL_data_dir, "whole_genome/freq_bins/freq_bin_{freq_bin}_all.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            df = dd.read_csv(input.files, sep = "\t", dtype = 'float64')
            df = df.compute()
            column_list = [str(x) for x in range(int(wildcards.freq_bin))]
            df_group = df.groupby("mu")[column_list].sum().reset_index()
            
            df_group = df_group.set_index('mu')
            sum_list = df_group.sum(axis = 1)
            df_group = df_group.div(df_group.sum(axis=1), axis=0)
            df_group["sum"] = sum_list
            df_group = df_group.reset_index()
            
            df_group.to_csv(output[0], sep = "\t", index = None)

####################################################### calculate multinomial p #######################################################

rule calculate_p:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/freq_bins/{chrom}/_metadata"),
        neutral_sfs_9 = os.path.join(KL_data_dir, "whole_genome/freq_bins/freq_bin_9_all.tsv"),
        # neutral_sfs_2 = os.path.join(KL_data_dir, "whole_genome/freq_bins/freq_bin_2_all.tsv"),
    output:
        os.path.join(scratch_dir, "whole_genome/p_9/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-10:00",
        cpus_per_task=2,
        mem_mb=30000
    run:
        with Client() as client:
            
            filename = input.rate
            rate = dd.read_parquet(filename.split("_metadata")[0])
            rate["Freq_bin_9"] = rate["Freq_bin_9"].astype(int)
            rate["Freq_bin_10"] = rate["Freq_bin_10"].astype(int)

            # don't put it in for loop b/c it is in dask
            # first start with bin_number 9
            filename = input.neutral_sfs_9
            neutral_sfs = pd.read_csv(filename, sep = "\t")
            neutral_sfs = neutral_sfs.drop(columns = ["mu", "sum"]).to_numpy()
            p_calculator_9 = mlr.multinomial_SFS_p(neutral_sfs, transformation = "none", reverse = False)

            bin_number = 9

            def get_p(df):
                return p_calculator_9.get_p(df["mu_index"], df[f"Freq_bin_{bin_number}"])

            rate[f"p_{bin_number}"] = rate.map_partitions(get_p)
            # rate[f"p_{bin_number}"] = rate.apply(lambda x : p_calculator_9.get_p(x["mu_index"], x[f"Freq_bin_{bin_number}"]), axis = 1)

            rate[f"E[p_{bin_number}]"] = rate.mu_index.map_partitions(lambda x: p_calculator_9.get_expected_p(x))
            rate[f"Var[p_{bin_number}]"] = rate.mu_index.map_partitions(lambda x: p_calculator_9.get_var_p(x))
            
            # rate[f"E[p_{bin_number}]"] = rate.apply(lambda x : p_calculator_9.get_expected_p(x["mu_index"]), axis = 1)
            # rate[f"Var[p_{bin_number}]"] = rate.apply(lambda x : p_calculator_9.get_var_p(x["mu_index"]), axis = 1)
                    
            rate = rate.repartition(partition_size="3GB")

            rate.to_parquet(output[0].split("_metadata")[0], write_index = True, compression = "gzip", write_metadata_file = True)

rule calculate_p_2:
    input:
        rate = os.path.join(scratch_dir, "whole_genome/p_9/{chrom}/_metadata"),
        # neutral_sfs_9 = os.path.join(KL_data_dir, "whole_genome/freq_bins/freq_bin_9_all.tsv"),
        neutral_sfs_2 = os.path.join(KL_data_dir, "whole_genome/freq_bins/freq_bin_2_all.tsv"),
    output:
        os.path.join(KL_data_dir, "whole_genome/p/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-10:00",
        cpus_per_task=2,
        mem_mb=24000
    run:
        with Client() as client:
            
            filename = input.rate
            rate = dd.read_parquet(filename.split("_metadata")[0])
            # don't put it in for loop b/c it is in dask
            # do for bin_number 2
            filename = input.neutral_sfs_2
            neutral_sfs = pd.read_csv(filename, sep = "\t")
            neutral_sfs = neutral_sfs.drop(columns = ["mu", "sum"]).to_numpy()
            p_calculator_2 = mlr.multinomial_SFS_p(neutral_sfs, transformation = "none", reverse = False)

            def get_p(df):
                return p_calculator_2.get_p(df["mu_index"], df["polymorphic"])
                
            rate["p_2"] = rate.map_partitions(get_p)
            rate["E[p_2]"] = rate.mu_index.map_partitions(lambda x: p_calculator_2.get_expected_p(x))
            rate["Var[p_2]"] = rate.mu_index.map_partitions(lambda x: p_calculator_2.get_var_p(x))

            # rate["p_2"] = rate.apply(lambda x : p_calculator_2.get_p(x["mu_index"], x["polymorphic"]), axis = 1)
            # rate["E[p_2]"] = rate.apply(lambda x : p_calculator_2.get_expected_p(x["mu_index"]), axis = 1)
            # rate["Var[p_2]"] = rate.apply(lambda x : p_calculator_2.get_var_p(x["mu_index"]), axis = 1)
                    
            rate = rate.repartition(partition_size="3GB")

            rate.to_parquet(output[0].split("_metadata")[0], write_index = True, compression = "gzip", write_metadata_file = True)
            
#############################################################################################################################
#############################################################################################################################


#For Features


#############################################################################################################################
#############################################################################################################################
phastcons_dir = "/n/data2/hms/dbmi/sunyaev/lab/igvf/zoonomia_phastcons_primates"

rule unzip_zoonomia_phastcons_primates:
    input:
        os.path.join(phastcons_dir, "chr{chrom}.bed.gz")
    output:
        os.path.join(scratch_dir, "chr{chrom}.bed"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=3000
    shell:
        "gunzip -c {input} > {output}"

rule add_zoonomia_phastcons_primates:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/p/{chrom}/_metadata"),
        zoo = os.path.join(scratch_dir, "chr{chrom}.bed")
    output:
        os.path.join(KL_data_dir, "whole_genome/zoonomia_phastcons_primates/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=28000
    run:
        with Client() as client:
            filename = input.rate
            rate = dd.read_parquet(filename.split("_metadata")[0])

            columns = ["Chrom", "Pos", "chromEnd", "name", "Zoonomia_phastcons_primates"]
            zoo = dd.read_csv(input.zoo, names = columns, sep = "\t", usecols=["Pos", "Zoonomia_phastcons_primates"],
                  dtype={'Pos': 'Int64', 'Zoonomia_phastcons_primates': "float"}, blocksize=100e6)

            print("Number of partitions: ", zoo.npartitions)
            zoo = zoo.set_index("Pos")

            #rather than merging, try dict
            # zoo = zoo.set_index("Pos")
            # zoo_dict = zoo.to_dict()
            # rate["Zoonomia_phastcons_primates"] = rate.Pos.map_partitions(lambda x: zoo_dict['Zoonomia_phastcons_primates'][x], 
            #                                                               meta=pd.Series(dtype="float64"))
            
            
            # zoo["Pos"] = zoo["Pos"].astype('Int64')
            # zoo_ddf = from_pandas(zoo, chunksize=36520314)
            rate = rate.merge(zoo, how = "left", right_index = True, left_on = "Pos")
            
            rate = rate.repartition(partition_size="1GB")
            
            rate.to_parquet(output[0].split("_metadata")[0], write_index = False, compression = "gzip", write_metadata_file = True)

#############################################################################################################################
#############################################################################################################################


#For Epigenetic Features


#############################################################################################################################
#############################################################################################################################


####################################################### get spliceAI #####################################################

rule add_spliceAI_vcf:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/mu_filtered/{chrom}/_metadata"),
        spliceai = os.path.join(scratch_dir, "downloads/spliceai/spliceai_scores.raw.snv.chrom_{chrom}.hg38.vcf")
    output:
        os.path.join(scratch_dir, "whole_genome/spliceai/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-3:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            filename = input.rate
            
            rate = dd.read_parquet(filename.split("_metadata")[0])
            rate = rate.drop(["mu"], axis = 1)
            
            names = ["Chrom", "Pos", "ID", "Allele_ref", "Allele", "QUAL", "FILTER", "Spliceai_info"]
            ddf = dd.read_csv(input.spliceai, sep = "\t", names = names, comment = "#", dtype={'Pos': 'Int64'})
            
            rate = rate.merge(ddf[["Pos", "Allele_ref", "Allele", "Spliceai_info"]], on = ["Pos", "Allele_ref", "Allele"], how = "left")
                        
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
#################################################### get DHS & footprints ####################################################
### DHS

rule add_dhs:
    input:
        rate = os.path.join(scratch_dir, "whole_genome/spliceai/{chrom}/_metadata"),
        dhs = os.path.join(KL_data_dir, "DHS_Index_and_Vocabulary_hg38_WM20190703.txt.gz")
    output:
        os.path.join(scratch_dir, "whole_genome/dhs_core/{chrom}/_metadata")
    resources:
        partition="short",
        runtime="0-3:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
        
            def get_range(start, end):
                # since this seems like it's one-based
                return range(start, end + 1)
                        
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            rate = rate.reset_index()
    
            # then add DHS intensity
            dhs = pd.read_csv(input.dhs, sep = "\t", dtype={'identifier': str})
            dhs = dhs[dhs["seqname"] == "chr" + str(wildcards.chrom)]
            dhs_core = dhs[dhs["core_start"].isna() == False]
    
            dhs["Pos"] = dhs.apply(lambda row: get_range(row["start"],row["end"]), axis=1)
            dhs = dhs.explode('Pos')
    
            dhs["Pos"] = dhs["Pos"].astype(int)
    
            dhs = dhs.sort_values("mean_signal").drop_duplicates(subset =["Pos"], keep = 'last')
    
            dhs = dhs.rename({"mean_signal": "DHS_mean_signal", "identifier": "DHS_identifier", "component": "DHS_tissue", "numsamples": "DHS_sample"}, axis = 1)
    #             dhs = from_pandas(dhs, npartitions=3)
    
            dhs = dhs[["Pos", "DHS_mean_signal", "DHS_sample", "DHS_identifier", "DHS_tissue"]]
    
    
            ## add dhs core            
            dhs_core["core_start"] = dhs_core["core_start"].astype(int)
            dhs_core["core_end"] = dhs_core["core_end"].astype(int)
    
            dhs_core["Pos"] = dhs_core.apply(lambda row: get_range(row["core_start"],row["core_end"]), axis=1)
            dhs_core = dhs_core.explode('Pos')
            dhs_core["Pos"] = dhs_core["Pos"].astype(int)
            dhs_core = dhs_core.sort_values("mean_signal").drop_duplicates(subset =["Pos"], keep = 'last')
            dhs_core = dhs_core.rename({"mean_signal": "DHS_core_mean_signal"}, axis = 1)
            dhs_core = dhs_core[["Pos", "DHS_core_mean_signal"]]
    
            dhs = dhs.merge(dhs_core, on = "Pos", how = "left")
    
            rate = rate.merge(dhs, on = "Pos", how = "left")
            rate = rate.repartition(partition_size="3GB")
    
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
# ### footprints
# fp_dir = "/home/djl34/lab_pd/data/footprints"
# from dask.dataframe import from_pandas

# rule add_footprinting:
#     input:
#         rate = os.path.join(scratch_dir, "whole_genome/dhs_core/{chrom}/_metadata"),
#         fp = os.path.join(KL_data_dir, "footprints/consensus_footprints_and_collapsed_motifs_hg38.bed.gz"),
#     output:
#         os.path.join(KL_data_dir, "whole_genome/footprints/{chrom}/_metadata"),
#     resources:
#         partition="short",
#         runtime="0-3:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         from dask.distributed import Client

#         with Client() as client:
        
#             rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
#             fp_header = ['Chrom', 'start', 'end', 'identifier', 'mean_signal', 'num_samples', 'num_fps', 'width', 'summit_pos', 'core_start', 'core_end', 'motif_clusters']
            
#             fp = pd.read_csv(input.fp, sep = "\t", names = fp_header)
            
#             def get_range(start, end):
#                 return range(start + 1, end + 1)

#             fp["Pos"] = fp.apply(lambda row: get_range(row["start"],row["end"]), axis=1)

#             fp = fp.explode('Pos')
#             fp["Pos"] = fp["Pos"].astype(int)

#             fp = fp.sort_values("mean_signal").drop_duplicates(subset =["Pos"], keep = 'last')
            
#             fp = fp.rename({"num_samples": "footprint_samples", "mean_signal": "footprint_mean_signal", "motif_clusters": "footprint_motif_clusters", "identifier": "footprint_identifier"}, axis = 1)
            
#             fp = fp[["Pos", "footprint_mean_signal", "footprint_samples", "footprint_motif_clusters", "footprint_identifier"]]
            
#             rate = rate.merge(fp, on = "Pos", how = "left").set_index('index')
            
#             rate = rate.repartition(partition_size="3GB")
            
#             rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)


rule combined_footprint:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/freq_bins/{chrom}/_metadata"),
        fp = os.path.join(KL_data_dir, "footprints/consensus_footprints_and_collapsed_motifs_hg38.bed.gz"),
    output:
        os.path.join(KL_data_dir, "whole_genome/combined/footprints/{chrom}/_metadata")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=6,
        mem_mb=120000
    run:
        with Client() as client:
        
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            fp_header = ['Chrom', 'start', 'end', 'identifier', 'mean_signal', 'num_samples', 'num_fps', 'width', 'summit_pos', 'core_start', 'core_end', 'motif_clusters']
            
            fp = pd.read_csv(input.fp, sep = "\t", names = fp_header)
            
#             fp["Chrom"] = fp["Chrom"].astype("string")
            fp = fp[fp["Chrom"] == "chr" + wildcards.chrom]
            
            def get_range(start, end):
                return range(start + 1, end + 1)

            fp["Pos"] = fp.apply(lambda row: get_range(row["start"],row["end"]), axis=1)

            fp = fp.explode('Pos')
            fp["Pos"] = fp["Pos"].astype("Int64")

            fp = fp.sort_values("mean_signal").drop_duplicates(subset =["Pos"], keep = 'last')
            
            fp = fp.rename({"num_samples": "footprint_samples", "mean_signal": "footprint_mean_signal", "motif_clusters": "footprint_motif_clusters", "identifier": "footprint_identifier"}, axis = 1)
            
            fp = fp[["Pos", "footprint_mean_signal", "footprint_samples", "footprint_motif_clusters", "footprint_identifier"]]
            
            rate = rate.merge(fp, on = "Pos", how = "left")
            
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
# rule add_gene_consequence:
#     input:
#         rate = os.path.join(KL_data_dir, "whole_genome/combined/footprints/{chrom}/_metadata"),
#         feature = os.path.join(KL_data_dir, "whole_exome/vep/{chrom}_rate_v5.2_TFBS_correction_whole_exon.tsv")
#     output:
#         os.path.join(KL_data_dir, "whole_genome/combined/add_gene_info/{chrom}/_metadata")
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=4,
#         mem_mb=54000
#     run:
#         with Client() as client:
#             names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'Consequence', 'Gene', 'Transcript', 'Canonical']

#             vep = dd.read_csv(input.feature, sep = "\t", names = names_list, dtype={'Pos': 'Int64'})
#             vep = vep[vep["Canonical"] == "YES"]
#             vep = vep.drop(columns = ["Chrom"])
            
#             rate = dd.read_parquet(input.rate.split("_metadata")[0])
            
#             rate = rate.merge(vep, on = ['Pos', 'Allele_ref', 'Allele'], how = "left")
            
#             rate = rate.repartition(partition_size="3GB")
            
#             rate.to_parquet(output[0].split("_metadata")[0], write_index = True, compression = "gzip", write_metadata_file = True)


rule add_gene_consequence_to_csv:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/combined/wgs_ukbb_gnomAD/{chrom}/_metadata"),
    output:
        os.path.join(KL_data_dir, "whole_genome/combined/wgs_ukbb_gnomAD/{chrom}.tsv.gz")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=10000
    run:
        with Client() as client:
            rate = dd.read_parquet(input.rate.split("_metadata")[0])
            rate = rate[rate["mu"] > 1e-07/per_generation_factor]
            rate.to_csv(output[0], sep = "\t", index = None, compression = "gzip", single_file = True)

#############################################################################################################################
#############################################################################################################################


#For Windows


#############################################################################################################################
#############################################################################################################################
rule make_windows:
    input:
        os.path.join(KL_data_dir, "whole_genome/filtered/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/windows/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            min_pos = rate["Pos"].min().compute()
            
            rate["window_20bp"] = (rate["Pos"] - min_pos)/20
            
            rate["window_20bp"] = rate["window_20bp"].astype(int)
            
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = True, compression = "gzip", write_metadata_file = True)            

rule make_windows_sum:
    input:
        os.path.join(scratch_dir, "whole_genome/windows/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/windows_zscore/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            #calculate expected
            k = 1.2603906363074207
                       
            rate["polymorphic_prob"] = 1 - np.exp(-1 * k * rate["mu"])

            rate["polymorphic"] = rate["AF_gnomADv3"].astype(bool).astype(int)

            rate_poly = rate.groupby("window_1k")[["polymorphic", "polymorphic_prob"]].sum().compute()
            rate_poly["size"] = rate.groupby("window_1k").size().compute()
            rate_poly["obs-exp"] = rate_poly["polymorphic"] - rate_poly["polymorphic_prob"]
            rate_poly["(obs-exp)**2"] = rate_poly["obs-exp"]**2
            rate_poly["z_score"] = rate_poly["(obs-exp)**2"]/rate_poly["polymorphic_prob"]
            rate_poly["z_score"] = rate_poly["z_score"]**.5
            rate_poly["z_score"] = rate_poly["z_score"].where(rate_poly["obs-exp"] < 0 , -1 * rate_poly["z_score"])
            rate_poly["z_score"] = rate_poly["z_score"].where(rate_poly["size"] >= 1000 , None)
            rate_poly = rate_poly.reset_index()

            z_score_dict = dict(zip(rate_poly.window_1k, rate_poly.z_score))

            def get_zscore(window):
                return z_score_dict[window]

            rate["window_1k_zscore"] = rate["window_1k"].map(z_score_dict)
#             rate["window_1k_zscore_positive"] = rate["window_1k_zscore"].where(rate["window_1k_zscore"] > 0 , None)
            
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
rule make_windows_zscore:
    input:
        os.path.join(scratch_dir, "whole_genome/windows/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/windows_zscore/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            #calculate expected
            k = 1.2603906363074207
                       
            rate["polymorphic_prob"] = 1 - np.exp(-1 * k * rate["mu"])

            rate["polymorphic"] = rate["AF_gnomADv3"].astype(bool).astype(int)

            rate_poly = rate.groupby("window_1k")[["polymorphic", "polymorphic_prob"]].sum().compute()
            rate_poly["size"] = rate.groupby("window_1k").size().compute()
            rate_poly["obs-exp"] = rate_poly["polymorphic"] - rate_poly["polymorphic_prob"]
            rate_poly["(obs-exp)**2"] = rate_poly["obs-exp"]**2
            rate_poly["z_score"] = rate_poly["(obs-exp)**2"]/rate_poly["polymorphic_prob"]
            rate_poly["z_score"] = rate_poly["z_score"]**.5
            rate_poly["z_score"] = rate_poly["z_score"].where(rate_poly["obs-exp"] < 0 , -1 * rate_poly["z_score"])
            rate_poly["z_score"] = rate_poly["z_score"].where(rate_poly["size"] >= 1000 , None)
            rate_poly = rate_poly.reset_index()

            z_score_dict = dict(zip(rate_poly.window_1k, rate_poly.z_score))

            def get_zscore(window):
                return z_score_dict[window]

            rate["window_1k_zscore"] = rate["window_1k"].map(z_score_dict)
#             rate["window_1k_zscore_positive"] = rate["window_1k_zscore"].where(rate["window_1k_zscore"] > 0 , None)
            
            rate = rate.repartition(partition_size="3GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
                        
            
rule add_enhancer_module:
    input:
        os.path.join(scratch_dir, "whole_genome/nonsense/{chrom}/_metadata"),
    output:
        os.path.join(KL_data_dir, "whole_genome/enhancer/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            df = pd.read_csv(KL_data_dir + "/epimap/epimap_enhancer_module.bed", sep = "\t", names = ["Chrom", "start", "end", "e_module"])
            
            df_chr = df[df["Chrom"] == "chr" + wildcards.chrom]
            
            def get_range(start, end):
                return range(start, end + 1)

            df_chr["Pos"] = df_chr.apply(lambda row: get_range(row["start"],row["end"]), axis=1)
            df_chr = df_chr.explode('Pos')
            df_chr["Pos"] = df_chr["Pos"].astype(int)
            df_chr = df_chr.drop(["start", "end", "Chrom"], axis = 1)
            
            rate = rate.merge(df_chr, on = "Pos", how = "left")
            
            rate = rate.repartition(partition_size="4GB")
            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
####################################################### add features #######################################################
###zoonomia phyloP

#maximum is 8.903
#minimum is -20
rule add_zoonomia:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/exon/{chrom}/_metadata"),
        zoo = os.path.join(KL_data_dir, "zoonomia/cactus241way/cactus241way.phyloP_chr{chrom}.wig")
    output:
        os.path.join(scratch_dir, "whole_genome/phylop/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        from dask.distributed import Client

        with Client() as client:
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")

            zoo = pd.read_csv(input.zoo, sep = "\t", names = ["Pos", "phyloP"], dtype={'Pos': 'int', 'phyloP': 'float64'})
#             zoo = zoo.repartition(partition_size="1GB")

            rate = rate.merge(zoo, on = "Pos", how = "left")

            rate["phyloP"] = rate["phyloP"].fillna(0)
            rate["phyloP_pos"] = rate["phyloP"].where(rate["phyloP"] > 0, 0)

            rate = rate.repartition(partition_size="3GB")

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

            
# rule normalize_zoonomia:
#     input:
#         rate = os.path.join(scratch_dir, "whole_genome/phylop/{chrom}/_metadata"),
#     output:
#         os.path.join(scratch_dir, "whole_genome/phylop_normalized/{chrom}/_metadata"),
#     run:
#         from dask.distributed import Client

#         with Client() as client:
#             rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
#             #normalize phyloP
#             rate["phyloP_normalized"] = rate["phyloP"]/8.903
            
#             factor = 20/8.903
#             rate["phyloP_normalized"] = rate["phyloP_normalized"].where(rate["phyloP_normalized"] > 0, rate["phyloP_normalized"]/factor)
#             rate["phyloP_normalized_pos"] = rate["phyloP_normalized"].where(rate["phyloP_normalized"] > 0, 0)
            
#             rate = rate.repartition(partition_size="3GB")
            
#             rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)




####################################################### add local info #######################################################

# add closest TSS
rule get_closest_TSS:
    input:
        os.path.join(KL_data_dir, "whole_genome/spliceai/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/closest_tss/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        from dask.distributed import Client

        with Client() as client:
            
            # first add TSS and gene
        
            biomart = pd.read_csv(pd_data_dir + "/biomart/ENSG_TSS_107.tsv.gz", sep = "\t")
            biomart_chr = biomart[biomart["Chromosome/scaffold name"] == wildcards.chrom]
            biomart_chr = biomart_chr[["Gene stable ID", "Transcription start site (TSS)"]].drop_duplicates()

            biomart_chr["Gene stable ID"] = biomart_chr.groupby("Transcription start site (TSS)")["Gene stable ID"].transform(lambda x: ','.join(x))
            biomart_chr = biomart_chr.drop_duplicates()

            biomart_chr = biomart_chr.sort_values("Transcription start site (TSS)").reset_index(drop = True)

            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            tss_list = list(biomart_chr["Transcription start site (TSS)"])
            tss_list.sort()

            mean_list = [(a + b) / 2 for a, b in zip(tss_list[:-1], tss_list[1:])]

            mean_list.insert(0, 0)

            if mean_list[-1] < rate["Pos"].max().compute():
                mean_list.append(rate["Pos"].max().compute() + 100000)

            rate['TSS_bin'] = rate["Pos"].map_partitions(pd.cut, mean_list, labels = False)

            biomart_dictionary = biomart_chr.to_dict()

            rate['TSS_Gene'] = rate['TSS_bin'].map(biomart_dictionary["Gene stable ID"])  
            rate['TSS_Pos'] = rate['TSS_bin'].map(biomart_dictionary["Transcription start site (TSS)"])
            rate['TSS_Distance'] = rate["Pos"] - rate['TSS_Pos'] 

            rate = rate.drop(['TSS_bin', 'TSS_Pos'], axis=1) 
            
            rate = rate.repartition(partition_size="3GB")            

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)


rule add_nonsense_mutations:
    input:
        os.path.join(scratch_dir, "whole_genome/closest_tss/{chrom}/_metadata"),
        "/home/djl34/lab_pd/aso/data/fake_transcript_variants_v4_lof_HC_filtered_unint.tsv"        
    output:
        os.path.join(scratch_dir, "whole_genome/nonsense/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=30000
    run:
        from dask.distributed import Client, LocalCluster
#         # set up cluster and workers
        cluster = LocalCluster()

        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            df = pd.read_csv(input[1], sep = "\t")

            df = df[["Chrom", "Pos", "Allele", "Gene", "Consequence"]]
            
            df["Chrom"] = df["Chrom"].astype(str)
            
            df = df[df["Chrom"] == wildcards.chrom]

            df = df.rename({"Gene": "Nonsense_Gene", "Consequence": "Nonsense_Consequence"}, axis = 1)
            
            rate = rate.merge(df, on = ["Pos", "Allele"], how = "left")

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
# rule add_KL_pergene:
#     input:
#         os.path.join(scratch_dir, "whole_genome/closest_tss/{chrom}/_metadata"),
#     output:
#         os.path.join(scratch_dir, "whole_genome/kl_pergene/{chrom}/_metadata"),
#     run:
#         with Client() as client:
            
#             rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/part.*.parquet")
            
#             df = pd.read_csv(KL_data_dir + "/lof_KL_pseudocount_1.tsv", sep = "\t")

#             df = df[["Gene", "KL"]]

#             df = df.rename({"Gene": "TSS_Gene", "KL": "TSS_KL"}, axis = 1)
            
#             rate = rate.merge(df, on = ["TSS_Gene"], how = "left")

#             rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

############################################## bin by allele frequency ##############################################
            
freq_breaks_5_bins = [-1, 1e-8, 0.00005, 0.0005, 0.05, 0.5]

#AN for gnomAD v3.1 is 152312, half is 76156

rule make_5bins:
    input:
        os.path.join(scratch_dir, "{header}/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "{header}/5_bins/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/part.*.parquet")
            
            rate["Freq_bin_5"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_5_bins, labels = False)
            
            rate = rate.repartition(partition_size="3GB")

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
        

rule make_5bins_SFS_neutral:
    input:
        os.path.join(scratch_dir, "{header}/5_bins/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "{header}/5_bins/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            df = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/part.*.parquet")
            
            df = df[["mu_newbin", "Freq_bin_5", "Neutral"]]
            df = df[df["Neutral"] == 1]
            df = df.compute()
            
            df["mu"] = df["mu_newbin"]

            df_group = pd.DataFrame(df.groupby(["mu", "Freq_bin_5"])["Freq_bin_5"].count())

            df_group = df_group.rename({"Freq_bin_5": "count"}, axis = 1)

            df_group = df_group.reset_index()

            df_group["count"] = df_group["count"].astype(int)

            df_group_pivot = df_group.pivot(index='mu', columns='Freq_bin_5', values='count')
            
            df_group_pivot = df_group_pivot.reset_index()
            
            df_group_pivot.to_csv(output[0], sep = "\t", index = None)            

rule make_adaptive_SFS_neutral:
    input:
        os.path.join(scratch_dir, "{header}/adaptive_bins/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "{header}/adaptive_bins/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            df = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/part.*.parquet")
            
            df = df[["mu_newbin", "Freq_bin_adaptive", "Neutral"]]
            df = df[df["Neutral"] == 1]
            df = df.compute()
            
            df["mu"] = df["mu_newbin"]

            df_group = pd.DataFrame(df.groupby(["mu", "Freq_bin_adaptive"])["Freq_bin_adaptive"].count())

            df_group = df_group.rename({"Freq_bin_adaptive": "count"}, axis = 1)

            df_group = df_group.reset_index()

            df_group["count"] = df_group["count"].astype(int)

            df_group_pivot = df_group.pivot(index='mu', columns='Freq_bin_adaptive', values='count')
            
            df_group_pivot = df_group_pivot.reset_index()
            
            df_group_pivot.to_csv(output[0], sep = "\t", index = None)
            
rule combine_5bins_SFS_neutral:
    input:
        files = [os.path.join(scratch_dir, "{header}/5_bins/" + chrom + ".tsv") for chrom in chrom_set]
    output:
        os.path.join(KL_data_dir, "{header}/5_bins/all.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            df = dd.read_csv(input.files, sep = "\t", dtype={'0.0': 'float64',
                           '1.0': 'float64',
                           '2.0': 'float64',
                           '3.0': 'float64',
                           '4.0': 'float64'})
    
            df = df.compute()
            
            df_group = df.groupby("mu")['0.0', '1.0', '2.0', '3.0', '4.0'].sum().reset_index()
            
            df_group = df_group.set_index('mu')
            sum_list = df_group.sum(axis = 1)
            df_group = df_group.div(df_group.sum(axis=1), axis=0)
            df_group["sum"] = sum_list
            df_group = df_group.reset_index()
            
            df_group.to_csv(output[0], sep = "\t", index = None)
            
rule combine_adaptive_SFS_neutral:
    input:
        files = [os.path.join(scratch_dir, "{header}/adaptive_bins/" + chrom + ".tsv") for chrom in chrom_set]
    output:
        os.path.join(KL_data_dir, "{header}/adaptive_bins/all.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            df = dd.read_csv(input.files, sep = "\t", dtype={'0.0': 'float64',
                           '1.0': 'float64', '2.0': 'float64', '3.0': 'float64',
                           '4.0': 'float64', '5.0': 'float64', '6.0': 'float64',
                           '7.0': 'float64', '8.0': 'float64', '9.0': 'float64'})
    
            df = df.compute()
            
            df_group = df.groupby("mu")['0.0', '1.0', '2.0','3.0','4.0','5.0','6.0','7.0','8.0','9.0'].sum().reset_index()
            
            df_group = df_group.set_index('mu')
            sum_list = df_group.sum(axis = 1)
            df_group = df_group.div(df_group.sum(axis=1), axis=0)
            df_group["sum"] = sum_list
            df_group = df_group.reset_index()
            
            df_group.to_csv(output[0], sep = "\t", index = None)
            
freq_breaks_2_bins = [-1, 1e-8, 0.5]

rule make_2bins:
    input:
        os.path.join(scratch_dir, "whole_genome/nonsense/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/nonsense/2_bins/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/part.*.parquet")
            
            rate["Freq_bin_2"] = rate['MAF'].map_partitions(pd.cut, freq_breaks_2_bins, labels = False)
            
            rate = rate.repartition(partition_size="3GB")

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
rule make_2bins_SFS_neutral:
    input:
        os.path.join(scratch_dir, "whole_genome/nonsense/2_bins/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/nonsense/2_bins/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            df = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/part.*.parquet")
            
            df = df[["mu_newbin", "Freq_bin_2", "Neutral"]]
            df = df[df["Neutral"] == 1]
            df = df.compute()
            
            df["mu"] = df["mu_newbin"]

            df_group = pd.DataFrame(df.groupby(["mu", "Freq_bin_2"])["Freq_bin_2"].count())

            df_group = df_group.rename({"Freq_bin_2": "count"}, axis = 1)

            df_group = df_group.reset_index()

            df_group["count"] = df_group["count"].astype(int)

            df_group_pivot = df_group.pivot(index='mu', columns='Freq_bin_2', values='count')
            
            df_group_pivot = df_group_pivot.reset_index()
            
            df_group_pivot.to_csv(output[0], sep = "\t", index = None)
            
rule combine_2_SFS_neutral:
    input:
        files = [os.path.join(scratch_dir, "whole_genome/nonsense/2_bins/" + chrom + ".tsv") for chrom in chrom_set]
    output:
        os.path.join(KL_data_dir, "whole_genome/neutral/2_bins/all.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            df = dd.read_csv(input.files, sep = "\t", dtype={'0.0': 'float64',
                           '1.0': 'float64', '2.0': 'float64', '3.0': 'float64',
                           '4.0': 'float64', '5.0': 'float64', '6.0': 'float64',
                           '7.0': 'float64', '8.0': 'float64', '9.0': 'float64'})
    
            df = df.compute()
            
            df_group = df.groupby("mu")['0.0', '1.0'].sum().reset_index()
            
            df_group = df_group.set_index('mu')
            sum_list = df_group.sum(axis = 1)
            df_group = df_group.div(df_group.sum(axis=1), axis=0)
            df_group["sum"] = sum_list
            df_group = df_group.reset_index()
            
            df_group.to_csv(output[0], sep = "\t", index = None)
