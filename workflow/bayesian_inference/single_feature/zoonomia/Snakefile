import os
import sys
import glob
import numpy as np
import pandas as pd
import math
import sys
import random
import pickle
import csv

import dask.dataframe as dd
from dask.distributed import Client

import torch
import pyro
import pyro.distributions as dist
import pyro.distributions.constraints as constraints
from pyro.nn import PyroModule

from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

sys.path.insert(0, '/home/djl34/lab_pd/kl/git/KL/scripts')
import raklette
from run_raklette import run_raklette
from run_raklette import run_raklette_cov
from run_raklette import TSVDataset

sys.path.insert(0, '/home/djl34/lab_pd/simulator/code')
from others import round_sig

def get_mem_mb(wildcards, attempt):
    return attempt * 25000

###################################################################################################################

KL_data_dir = "/home/djl34/lab_pd/kl/data"
scratch_dir = "/n/scratch3/users/d/djl34"

base_set = ["A", "C", "T", "G"]
all_chrom_set = [str(x) for x in range(1, 23)]
even_chrom_set = [str(2 * x) for x in range(1, 12)]
# chrom_set = even_chrom_set
# chrom_set = ["2"]
chrom_set = ["-2"]

# cutoff_list = [0, 1, 2, 3, 4, 5]

wildcard_constraints:
    chrom="\d+",
    chrom_all="[-+]?\d+",
    epoch="\d+",
    interval_min="[+-]?([0-9]*[.])?[0-9]+",
    interval_max="[+-]?([0-9]*[.])?[0-9]+",
    samplesize="\d+",

file_directory = "single_feature/zoonomia/"

###################################################################################################################
header_names = ["zoonomia_"]

interval_list = []

interval_list = range(20)
interval_list = [x/20 for x in interval_list]

interval_list = [str(x) + "_" + str(round_sig(x + 0.05, 2)) for x in interval_list]

# interval_list = interval_list[:1]

print(interval_list)


covprior_list = [0.1, 1.0, 10.0]
covprior_list = [0.1]

learning_rate_list = [0.01, 0.1, 1.0]
learning_rate_list = [0.02]

rule all:
    input:
## for intervals
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "-2.tsv"),
#         [os.path.join(scratch_dir, "kl_input/" + file_directory + "gene_edges_"+ interval +"_only_" + chrom + ".tsv") for chrom in chrom_set for interval in interval_list],
#         [os.path.join(KL_data_dir, "raklette_output/" + file_directory + "gene_edges_"+ interval +"_only_" + chrom + "_sample_100000000_covonly_lr_0.01_gamma_0.5_chunksize_100000_epoch_5_covprior_0.1.pkl") for chrom in chrom_set for interval in interval_list],
## for covariates
        [os.path.join(KL_data_dir, "raklette_output/" + file_directory + "phyloP_phyloPsq_phylo6_chr_" + chrom + "_chunksize_1000000_covonly_lr_0.01_gamma_0.5_epoch_1_covprior_0.1.pkl") for chrom in chrom_set],

########################################## zoonomia genes ##############################################

rule make_tsv_files:
    input:
        os.path.join(scratch_dir, "whole_genome/phylop/{chrom}/_metadata")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=4,
        mem_mb=get_mem_mb
    run:
        with Client() as client:            
            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            print(rate.columns)

            include_columns = ["mu_index", "Freq_bin_adaptive", "phyloP_pos"]

            rate = rate[include_columns]

            rate = rate[rate["phyloP_pos"] > 0]

            rate = rate.rename(columns={"phyloP_pos": "phyloP", "Freq_bin_adaptive": "Freq_bin"})  
            rate["phyloP"] = rate["phyloP"]/8.903

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
                
# rule make_tsv_files_even:
#     input:
#         [os.path.join(scratch_dir, "kl_input/" + file_directory + chrom + ".tsv") for chrom in even_chrom_set]
#     output:
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "-2.tsv")
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=5,
#         mem_mb=25000
#     run:
#         with Client() as client:

#             rate = dd.read_csv(input, sep = "\t")

#             rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
            
rule make_phyloP6:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "phyloP_phylo6_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=4,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_csv(input, sep = "\t")

            print(rate.columns)

            include_columns = ["mu_index", "Freq_bin", "phyloP", "phyloP6"]
            
            rate["phyloP6"] = rate["phyloP"].where((rate["phyloP"] >= 6.0/8.903) & (rate["phyloP"] < 6.4/8.903), 0)
            rate["phyloP6"] = rate["phyloP6"].astype(bool).astype(int)
            
            rate = rate[include_columns]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
            
rule make_phyloPsq_phyloP6:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "phyloP_phyloPsq_phylo6_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=4,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_csv(input, sep = "\t")

            print(rate.columns)

            include_columns = ["mu_index", "Freq_bin", "phyloP", "phyloP^2", "phyloP6"]
            
            rate["phyloP6"] = rate["phyloP"].where((rate["phyloP"] >= 6.0/8.903) & (rate["phyloP"] < 6.4/8.903), 0)
            rate["phyloP6"] = rate["phyloP6"].astype(bool).astype(int)
            rate["phyloP^2"] = rate["phyloP"] ** 2

            rate = rate[include_columns]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
    
rule zoonomia_make_genes_interval_by_edges_only:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "gene_edges_{interval_min}_{interval_max}_only_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=20000
    run:
        with Client() as client:

            rate = dd.read_csv(input[0], sep = "\t")

            print(rate.columns)
            
            include_columns = ["mu_index", "Freq_bin"]
            
            interval_min = float(wildcards.interval_min)
            interval_max = float(wildcards.interval_max)
            
            column_name = "phyloP_" + str(interval_min) + "_to_" + str(interval_max)
            
            include_columns.append(column_name)
                
            rate[column_name] = rate["phyloP"].where((rate["phyloP"] >= interval_min) & (rate["phyloP"] < interval_max), 0)
            rate[column_name] = rate[column_name].astype(bool).astype(int)
            
            rate = rate[rate[column_name] == 1]
            
            rate = rate[include_columns]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
##############################################################################################################################
# rule sort:
#     input:
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}.tsv")
#     output:
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "/{chrom}_sorted.tsv")
#     run:
#         with Client() as client:
#             rate = dd.read_csv(input[0], sep = "\t")
#             rate = rate.sort_values('window_1k') 
#             rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
                

rule sample_size:
    input:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}.tsv")
    output:
        first = os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_sample_{samplesize}_sampled.tsv"),
        second = os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_sample_{samplesize}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            rate = dd.read_csv(input[0], sep = "\t")
            
            sites = len(rate)
            
            if sites != 0:
                rate = rate.sample(frac = int(wildcards.samplesize)/sites, replace = True)
            
            rate.to_csv(output.first, sep = "\t", index = None, single_file = True)
            
            head_value = int(wildcards.samplesize) + 1
            
            shell("head -n {head_value} {output.first} >> {output.second}")
                        
             
            
rule make_chunks:
    input:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_chr_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_chr_{chrom}/chunk_{chunksize}_0.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=5000
    run:
        #remove previous files
        header = "_".join(output[0].split("_")[:-1])
        header = header + "*.tsv"
        try:
            shell("rm " + header)
        except:
            print("no file")
        print("removing " + header)
                
        iterator = 0
        
        input_filename = input[0]
        output_dir = "/".join(output[0].split("/")[:-1])
        
        chunksize = int(wildcards.chunksize)
        
        with pd.read_csv(input_filename, chunksize=chunksize, sep = "\t") as reader:
            row = 0
            for chunk in reader:
                print(iterator)
                chunk.to_csv(output_dir + "/chunk_" + str(chunksize) + "_" + str(iterator) + ".tsv", sep = "\t", index = None)
                iterator += 1

rule make_tsv_files_even:
    input:
        [os.path.join(scratch_dir, "kl_input/" + file_directory + "{header}_chr_" + chrom + "/chunk_{chunksize}_0.tsv") for chrom in even_chrom_set]
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{header}_chr_-2/chunk_{chunksize}_0.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        #remove previous files
        header = "_".join(output[0].split("_")[:-1])
        header = header + "*.tsv"
        try:
            shell("rm " + header)
        except:
            print("no file")
        print("removing " + header)
        
        output_dir = "/".join(output[0].split("/")[:-1])
        iterator = 0
        
        for filename in input:
            file_list = glob.glob("_".join(filename.split("_")[:-1]) + "_*.tsv")
            
            for file in file_list:
                rate = pd.read_csv(file, sep = "\t")
            
                rate.to_csv(output_dir + "/chunk_" + str(wildcards.chunksize) + "_" + str(iterator) + ".tsv", sep = "\t", index = None)
                iterator += 1
            
rule get_length:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{header}_chr_{chrom_all}/chunk_{chunksize}_0.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_chr_{chrom_all}_chunk_{chunksize}_length.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=4,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            input_filename = "_".join(input[0].split("_")[:-1]) + "_*.tsv"
            
            rate = dd.read_csv(input_filename, sep = "\t")
            
            length = len(rate)
            column_length = len(rate.columns)
            
            f = open(output[0], "w")
            
            f.write(str(length) + "\n")
            f.write(str(column_length) + "\n")
            
            f.close()     
########################################## For running KL analysis #####################################
        
rule run_KL_cov:
    input:
        variants = os.path.join(scratch_dir, "kl_input/" + file_directory + "{header}_chr_{chrom_all}/chunk_{chunksize}_0.tsv"),
        length_file = os.path.join(scratch_dir, "kl_input/" + file_directory +"{header}_chr_{chrom_all}_chunk_{chunksize}_length.tsv"),
        neutral_sfs = os.path.join(KL_data_dir, "whole_genome/allele_freq/adaptive_bins/all.tsv"),
#         neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all.tsv",
#         neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all_original.tsv"
    output:
        os.path.join(KL_data_dir, "raklette_output/"+ file_directory +"{header}_chr_{chrom_all}_chunksize_{chunksize}_covonly_lr_{learning_rate}_gamma_{gamma}_epoch_{epoch}_covprior_{cov_prior}.pkl")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=10,
        mem_mb=20000
    run:
        n_covs = 1
        
        input_filename = input.variants
        input_directory = "/".join(input_filename.split("/")[:-1]) + "/"
        output_filename = output[0]
        neutral_sfs_filename = input.neutral_sfs
        
        df = pd.read_csv(input.length_file, sep = "\t", header = None)
        nb_samples = df[0][0]
        nb_features = df[0][1] - 2
        
        print("number of samples: " + str(nb_samples))
        
        if nb_samples == 0:
            f = open(output_filename, "w")
            f.write("no sample")
            f.close()
        else:        
            with open(input.variants) as f:
                first_line = f.readline()
            header = first_line.split("\t")
            
            chunksize = int(wildcards.chunksize)

            print("number of chunks " + str(nb_samples/chunksize))
            
            dataset = TSVDataset(input_directory, chunksize=chunksize, nb_samples = nb_samples, header_all = header, features = header)
            loader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False)

            num_epochs = int(wildcards.epoch)
            cov_prior = float(wildcards.cov_prior)
            
            #lets run raklette
            run_raklette_cov(loader, nb_features, num_epochs, neutral_sfs_filename, output_filename, 
                         float(wildcards.learning_rate), float(wildcards.gamma), 
                             cov_sigma_prior = torch.tensor(cov_prior, dtype=torch.float32))
           
            