import sys
sys.path.insert(0,'/home/djl34/kl_git/scripts')
from snakefile_imports import *


wildcard_constraints:
    chrom="\d+"

n = 0

## function for allocating memory into 
def get_mem_mb(wildcards, attempt):
    return 10000 + (attempt + n) * 30000
    
print(chrom_set)

cwd = os.getcwd()
file_directory = "/".join(cwd.split("/")[-2:]) + "/"

include: "/home/djl34/kl_git/results/method_validation/Snakefile"

##############################################################################################################################   
## for epigenetic features
transform_list = ["sqrt", "log_plus_one", "no_common", "no_transform"]


# input_list = [os.path.join(KL_data_dir, "results/method_validation/footprints/{chrom}.tsv"),
#               os.path.join(KL_data_dir, "results/method_validation/footprints/KL_freq_bin_9_pseudocount_0_chrom_{chrom}.tsv"),
#               os.path.join(KL_data_dir, "results/method_validation/footprints/KL_freq_bin_2_pseudocount_0_chrom_{chrom}.tsv"),
#               os.path.join(KL_data_dir, "results/method_validation/footprints/pergene/mu_{chrom}.tsv"),
#               os.path.join(KL_data_dir, "results/method_validation/footprints/zoonomia/{chrom}.tsv"),
#               os.path.join(KL_data_dir, "results/method_validation/footprints/pergene/zoonomia_{chrom}.tsv"),
#               os.path.join(KL_data_dir, "results/method_validation/footprints/pergene/denovo_{chrom}.tsv"),
#               os.path.join(scratch_dir, "results/method_validation/footprints/zoonomia/KL_9_split_1_half_{chrom}.tsv"),
#               os.path.join(scratch_dir, "results/method_validation/footprints/zoonomia/KL_2_split_1_half_{chrom}.tsv")]

# input_list = [input_filename.format(chrom = chrom) for input_filename in input_list for chrom in chrom_set]



rule all:
    input:
        # input_list,
        os.path.join(KL_data_dir, "results/residual_variance/footprints/KL_2_split_1_half_AC_0.tsv"),
        os.path.join(KL_data_dir, "results/residual_variance/footprints/KL_2_split_2_half_AC_0.tsv"),
        os.path.join(KL_data_dir, "results/residual_variance/footprints/KL_9_split_1_half_AC_0.tsv"),
        os.path.join(KL_data_dir, "results/residual_variance/footprints/KL_9_split_2_half_AC_0.tsv"),
        # os.path.join(KL_data_dir, "results/method_validation/footprints/residual_variance_AC_3.tsv"),
        # os.path.join(KL_data_dir, "results/method_validation/footprints/residual_variance_AC_0_per_footprint.tsv")
        # [os.path.join(scratch_dir, f"results/footprints/top/top_region_{rank}.tsv")  for rank in [1000, 5000]]
        # [os.path.join(scratch_dir, f"results/footprints/top/top_{rank}.tsv") for rank in [1000, 5000]]

################################################# creating data for results ##################################################

# create data for footprints
rule make_footprints:
    input:
        os.path.join(KL_data_dir, "whole_genome/footprints/{chrom}/_metadata")
    output:
        os.path.join(scratch_dir, "results/footprints/split/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=3,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            filename = input[0]
            
            rate = dd.read_parquet("/".join(filename.split("/")[:-1]) + "/")
            rate["footprint_mean_signal"] = rate["footprint_mean_signal"].fillna(0)
            rate_footprints = rate[(rate["footprint_mean_signal"] > 0)]
            rate_footprints.to_csv(output[0], sep = "\t", single_file = True)

rule add_AC:
    input:
        rate = os.path.join(scratch_dir, "results/footprints/split/{chrom}.tsv"),
        ac = os.path.join(KL_data_dir, "whole_genome/freq_bins/{chrom}/_metadata"),
    output:
        os.path.join(KL_data_dir, "results/method_validation/footprints/{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=3,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object', 'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'})
            feature = dd.read_parquet(input.ac.split("_metadata")[0])
            rate = rate.drop(['mu'], axis=1)
            rate["Pos"] = rate["Pos"].astype("Int64")
            
            rate = rate.merge(feature, on = ['Pos', 'Allele_ref', 'Allele'], how = "inner")
            rate = rate[~rate["cds"]]
            rate = rate.drop_duplicates(subset = ["Pos", "Allele_ref", "Allele"])
            rate = rate.rename(columns = {"footprint_identifier": "region"})
            rate.to_csv(output[0], sep = "\t", single_file = True, index = None)

rule add_denovo:
    input:
        rate = os.path.join(KL_data_dir, "results/method_validation/footprints/{chrom}.tsv"),
        denovo = os.path.join(KL_data_dir, "de_novo/an_etal/aat6576_table-s2.csv"),
        denovo2 = os.path.join(KL_data_dir, "de_novo/halldorsson_etal/aau1043_datas5_revision1.tsv"),
    output:
        os.path.join(scratch_dir, "results/method_validation/footprints/denovo/{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=3,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object', 'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'})
            
            df = pd.read_csv(input.denovo)
            
            df = df.rename({"Alt": "Allele"}, axis = 1)
            
            df = df[df["Type"] == "SNV"]
            df = df[df["Chr"] == "chr" + wildcards.chrom]
            
            df_case = df[df["Pheno"] == "case"]
            df_control = df[df["Pheno"] == "control"]
            
            df_case["denovo_an_case"] = 1
            df_control["denovo_an_control"] = 1
            
            rate = rate.merge(df_case[["Pos", "Allele", "denovo_an_case"]], on = ["Pos", "Allele"], how = "left")
            rate = rate.merge(df_control[["Pos", "Allele", "denovo_an_control"]], on = ["Pos", "Allele"], how = "left")

            #for second control_file
            df = pd.read_csv(input.denovo2, sep = "\t", comment = "#")
            df = df.rename({"Alt": "Allele"}, axis = 1)
            df = df[df["Chr"] == f"chr{wildcards.chrom}"]

            df["denovo_halldorsson_control"] = 1
            rate = rate.merge(df[["Pos", "Allele", "denovo_halldorsson_control"]], on = ["Pos", "Allele"], how = "left")
            
            rate.to_csv(output[0], sep = "\t", single_file = True, index = None)
            
rule add_zoonomia_footprints:
    input:
        rate = os.path.join(scratch_dir, "results/method_validation/footprints/denovo/{chrom}.tsv"),
        zoo = os.path.join(KL_data_dir, "zoonomia/cactus241way/cactus241way.phyloP_chr{chrom}.wig")
    output:
        os.path.join(KL_data_dir, "results/method_validation/footprints/zoonomia/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=3,
        mem_mb=get_mem_mb
    run:
        from dask.distributed import Client

        with Client() as client:
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object', 'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'})

            zoo = pd.read_csv(input.zoo, sep = "\t", names = ["Pos", "phyloP"], dtype={'Pos': 'int', 'phyloP': 'float64'})
#             zoo = zoo.repartition(partition_size="1GB")

            rate = rate.merge(zoo, on = "Pos", how = "left")

            rate["phyloP"] = rate["phyloP"].fillna(0)
            rate["phyloP_pos"] = rate["phyloP"].where(rate["phyloP"] > 0, 0)

            rate.to_csv(output[0], sep = "\t", single_file = True, index = None)

########################################################### pergene analysis #########################################################
rule pergene_mu:
    input:
        rate = os.path.join(KL_data_dir, "results/method_validation/footprints/{chrom}.tsv"),
    output:
        os.path.join(KL_data_dir, "results/method_validation/footprints/pergene/mu_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=3000
    run:
        with Client() as client:
            
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object', 
                                                              'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'})

            rate["predicted_mu"] = rate["mu"] * per_generation_factor
            rate_gene = rate.groupby("region")["predicted_mu", "polymorphic"].sum().compute()
            rate_gene = pd.DataFrame(rate_gene)
            rate_gene["sites"] = rate.groupby("region").size().compute()
            rate_gene.to_csv(output[0], sep = "\t")
            
            
rule pergene_denovo:
    input:
        rate = os.path.join(scratch_dir, "results/method_validation/footprints/denovo/{chrom}.tsv"),
    output:
        os.path.join(KL_data_dir, "results/method_validation/footprints/pergene/denovo_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=2000
    run:
        with Client() as client:
            
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object', 
                                                              'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'})

            rate_gene = rate.groupby("region")[["denovo_an_case", "denovo_an_control"]].sum().compute()
            rate_gene = pd.DataFrame(rate_gene)
#             rate_gene["sites"] = rate.groupby("region").size().compute()
            rate_gene.to_csv(output[0], sep = "\t")

rule pergene_zoonomia:
    input:
        rate = os.path.join(KL_data_dir, "results/method_validation/footprints/zoonomia/{chrom}.tsv"),
    output:
        os.path.join(KL_data_dir, "results/method_validation/footprints/pergene/zoonomia_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=3000
    run:
        with Client() as client:
            
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object', 
                                                              'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'})

            phyloP_cutoff_list = [1, 3, 5, 7]
            phyloP_cutoff_header_list = [f"phyloP_{i}_cutoff" for i in phyloP_cutoff_list]

            for i in phyloP_cutoff_list:
                rate[f"phyloP_{i}_cutoff"] = rate["phyloP"] > i
                rate[f"phyloP_{i}_cutoff"] = rate[f"phyloP_{i}_cutoff"].astype(int)

            rate_gene = rate.groupby("region")[phyloP_cutoff_header_list].sum().compute()
            rate_gene = pd.DataFrame(rate_gene)
            rate_gene.to_csv(output[0], sep = "\t")

########################################################### split sites ############################################################
rule split_by_KL:
    input:
        rate = os.path.join(KL_data_dir, "results/method_validation/footprints/zoonomia/{chrom}.tsv"),
        kl = os.path.join(KL_data_dir, "results/method_validation/footprints/KL_freq_bin_{KL_bin_num}_pseudocount_0_chrom_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "results/method_validation/footprints/zoonomia/KL_{KL_bin_num}_split_1_half_{chrom}.tsv"),
        os.path.join(scratch_dir, "results/method_validation/footprints/zoonomia/KL_{KL_bin_num}_split_2_half_{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=2,
        mem_mb=4000
    run:
        from dask.distributed import Client

        with Client() as client:
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object', 'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'})

            kl = pd.read_csv(input.kl, sep = "\t")

            kl_column_name = "max_likelihood_KL"

            ## sort footprints by KL values in ascending order
            kl = kl.sort_values(kl_column_name, ascending = True)

            ## split by first half and second half of KL values
            kl_half1 = kl.iloc[:int(len(kl)/2)]
            kl_half2 = kl.iloc[int(len(kl)/2):]

            ## get the sites by the footprints
            rate_half1 = rate[rate["region"].isin(kl_half1["region"])]
            rate_half2 = rate[rate["region"].isin(kl_half2["region"])]
            
            rate_half1.to_csv(output[0], sep = "\t", single_file = True, index = None)
            rate_half2.to_csv(output[1], sep = "\t", single_file = True, index = None)


################################################# check for mutation rate misspecification ##################################################

rule SNP_ascertainment_var_estimation:
    input:
        files = [os.path.join(scratch_dir, "results/method_validation/footprints/zoonomia/{{header}}{chrom}.tsv").format(chrom = chrom) for chrom in all_chrom_set]
    output:
        os.path.join(KL_data_dir, "results/residual_variance/footprints/{header}AC_{AC}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=10000
    run:
        with Client() as client:

            ddf = dd.read_csv(input.files, sep = "\t", 
                              dtype={'Spliceai_info': 'object', 'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'},
                              usecols=['mu', 'AC_total', 'denovo_an_case', 'denovo_an_control', 'denovo_halldorsson_control'])
            ddf = ddf.fillna(0)
            ddf["denovo"] = ddf["denovo_an_case"] + ddf["denovo_an_control"] + ddf["denovo_halldorsson_control"]

            # try to see if I can reduce partition number to make computation faster
            ddf = ddf[["mu", "AC_total", "denovo"]].repartition(partition_size="300MB")

            ## get counts
            ac_cutoff = int(wildcards.AC)
            
            polymorphic_sum = ddf[ddf["AC_total"] > ac_cutoff].groupby("mu")["denovo"].sum().compute()
            
            sum = pd.DataFrame(polymorphic_sum)
            
            sum.rename({"denovo": "number_of_mutations_SNV"}, axis = 1, inplace = True)
            
            sum["number_of_sites_SNV"] = ddf[ddf["AC_total"] > ac_cutoff].groupby("mu").size().compute()
            sum["number_of_mutations_no_SNV"] = ddf[ddf["AC_total"] == ac_cutoff].groupby("mu")["denovo"].sum().compute()
            sum["number_of_sites_no_SNV"] = ddf[ddf["AC_total"] == ac_cutoff].groupby("mu").size().compute()
            
            sum["rate_SNV_div_rate_no_SNV"] = ((sum["number_of_mutations_SNV"]/sum["number_of_sites_SNV"])/
                                               (sum["number_of_mutations_no_SNV"]/sum["number_of_sites_no_SNV"]))

            sum.to_csv(output[0], sep = "\t")



