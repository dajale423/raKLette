import os
import sys
import glob
import numpy as np
import pandas as pd
import math
import sys
import random
import pickle
import csv

import dask.dataframe as dd
import dask.array as da
from dask.distributed import Client

import torch
import pyro
import pyro.distributions as dist
import pyro.distributions.constraints as constraints
from pyro.nn import PyroModule

from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

sys.path.insert(0, '/home/djl34/lab_pd/kl/git/KL/scripts')
import raklette
from run_raklette import run_raklette
from run_raklette import run_raklette_cov
from run_raklette import TSVDataset

sys.path.insert(0, '/home/djl34/lab_pd/simulator/code')
from others import round_sig

def get_mem_mb(wildcards, attempt):
    return attempt * 10000

def get_mem_mb_length(wildcards, attempt):
    return attempt * 5000

def get_mem_mb_raklette(wildcards, attempt):
    return int(wildcards.chunksize)/1000

###################################################################################################################

KL_data_dir = "/home/djl34/lab_pd/kl/data"
scratch_dir = "/n/scratch3/users/d/djl34"

base_set = ["A", "C", "T", "G"]
chrom_set = [str(x) for x in range(1, 23)]
chrom_set = ["2"]
# chrom_set = ["-2"]
# chrom_set = ["22"]


even_chrom_set = [str(2 * x) for x in range(1, 12)]
# chrom_set = even_chrom_set

# cutoff_list = [0, 1, 2, 3, 4, 5]

wildcard_constraints:
    chrom="\d+",
    chrom_all="[-+]?\d+",
    epoch="\d+",
    interval_min="[+-]?([0-9]*[.])?[0-9]+",
    interval_max="[+-]?([0-9]*[.])?[0-9]+",
    samplesize="\d+",

file_directory = "single_feature/closest_tss/"

###################################################################################################################

header_list = ["nothing_close_tss_", "nothing_far_tss_", "footprints_close_to_tss_", "footprints_far_tss_"]

rule all:
    input:
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_dhs_chr_22.tsv"),
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_dhs_gene_chr_22.tsv"),
#         os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_dhs_gene_chr_-2/chunk_100000_0.tsv")
        [os.path.join(KL_data_dir, "raklette_output/" + file_directory + header + "chr_" + chrom + "_chunksize_1000000_covonly_lr_0.01_gamma_0.5_epoch_10_covprior_0.1.pkl") for chrom in chrom_set for header in header_list],
#         [os.path.join(KL_data_dir, "raklette_output/" + file_directory + "nothing_far_tss_chr_" + chrom + "_chunksize_1000000_covonly_lr_0.01_gamma_0.5_epoch_10_covprior_0.1.pkl") for chrom in chrom_set],
        
########################################## getting column of interest ##############################################

rule make_tsv_file:
    input:
        os.path.join(scratch_dir, "whole_genome/closest_tss/{chrom}/_metadata")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "tss_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            print(rate.columns)

            include_columns = ["mu_index", "Freq_bin_adaptive"]
            feature_list = ["footprint_mean_signal", "DHS_mean_signal", "exon", "TSS_Distance"]
            include_columns.extend(feature_list)

            for feature in feature_list:
                rate[feature] = rate[feature].fillna(0)
                
            rate = rate[include_columns]
            
            rate = rate.rename(columns={"Freq_bin_adaptive": "Freq_bin"})  
            
            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
            
            
         
        
        
#         os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_chr_{chrom}.tsv")
#     output:
#         os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_chr_{chrom}/chunk_{chunksize}_0.tsv")

rule nothing_close_tss_only_gene:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "tss_chr_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "nothing_close_tss_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_csv(input[0], sep = "\t")
            
            rate["nothing_close"] = 1
            
            rate["nothing_close"] = rate["nothing_close"].where(((rate["footprint_mean_signal"] == 0) & (rate["DHS_mean_signal"] == 0) & (rate["exon"] == 0) & (rate["TSS_Distance"] < 5000)), 0)   
            
            cov_columns = ["nothing_close"]
            include_columns = ["mu_index", "Freq_bin"]
            include_columns.extend(cov_columns)

            rate = rate[include_columns]

            for column in cov_columns:
                rate[column] = rate[column].astype(bool).astype(int)

            rate = rate[rate[cov_columns[0]] == 1]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
rule nothing_far_tss_only_gene:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "tss_chr_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "nothing_far_tss_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_csv(input[0], sep = "\t")
            
            rate["nothing_far"] = 1
            
            rate["nothing_far"] = rate["nothing_far"].where(((rate["footprint_mean_signal"] == 0) & (rate["DHS_mean_signal"] == 0) & (rate["exon"] == 0) & (rate["TSS_Distance"] >= 5000)), 0) 
            
            cov_columns = ["nothing_far"]
            include_columns = ["mu_index", "Freq_bin"]
            include_columns.extend(cov_columns)

            rate = rate[include_columns]

            for column in cov_columns:
                rate[column] = rate[column].astype(bool).astype(int)

            rate = rate[rate[cov_columns[0]] == 1]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)

rule footprints_close_tss_only_gene:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "tss_chr_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_close_to_tss_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_csv(input[0], sep = "\t")
            
            rate["footprints_close"] = rate["footprint_mean_signal"].where((rate["footprint_mean_signal"] > 0) & (rate["TSS_Distance"] < 5000), 0)

            cov_columns = ["footprints_close"]
            include_columns = ["mu_index", "Freq_bin"]
            include_columns.extend(cov_columns)

            rate = rate[include_columns]

            for column in cov_columns:
                rate[column] = rate[column].astype(bool).astype(int)

            rate = rate[rate[cov_columns[0]] == 1]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            
rule footprints_far_tss_only_gene:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_tss_chr_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "footprints_far_tss_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_csv(input[0], sep = "\t")
            
            rate["footprints_far"] = rate["footprint_mean_signal"].where((rate["footprint_mean_signal"] > 0) & (rate["TSS_Distance"] > 5000), 0)

            cov_columns = ["footprints_far"]
            include_columns = ["mu_index", "Freq_bin"]
            include_columns.extend(cov_columns)

            rate = rate[include_columns]

            for column in cov_columns:
                rate[column] = rate[column].astype(bool).astype(int)

            rate = rate[rate[cov_columns[0]] == 1]

            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)
            

                        
##############################################################################################################################

                

rule sample_size:
    input:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_chr_{chrom}.tsv")
    output:
        first = os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_sample_{samplesize}_chr_{chrom}_sampled.tsv"),
        second = os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_sample_{samplesize}_chr_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            rate = dd.read_csv(input[0], sep = "\t")
            
            sites = len(rate)
            
            if sites != 0:
                rate = rate.sample(frac = int(wildcards.samplesize)/sites, replace = True)
            
            rate.to_csv(output.first, sep = "\t", index = None, single_file = True)
            
            head_value = int(wildcards.samplesize) + 1
            
            shell("head -n {head_value} {output.first} >> {output.second}")
                        
            
            
rule make_chunks:
    input:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_chr_{chrom}.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_chr_{chrom}/chunk_{chunksize}_0.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=700
    run:
        #remove previous files
        header = "_".join(output[0].split("_")[:-1])
        header = header + "*.tsv"
        try:
            shell("rm " + header)
        except:
            print("no file")
        print("removing " + header)
                
        iterator = 0
        
        input_filename = input[0]
        output_dir = "/".join(output[0].split("/")[:-1])
        
        chunksize = int(wildcards.chunksize)
        
        with pd.read_csv(input_filename, chunksize=chunksize, sep = "\t") as reader:
            row = 0
            for chunk in reader:
                print(iterator)
                chunk.to_csv(output_dir + "/chunk_" + str(chunksize) + "_" + str(iterator) + ".tsv", sep = "\t", index = None)
                iterator += 1
                           
rule make_tsv_files_even:
    input:
        [os.path.join(scratch_dir, "kl_input/" + file_directory + "{header}_chr_" + chrom + "/chunk_{chunksize}_0.tsv") for chrom in even_chrom_set]
    output:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{header}_chr_-2/chunk_{chunksize}_0.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        #remove previous files
        header = "_".join(output[0].split("_")[:-1])
        header = header + "*.tsv"
        try:
            shell("rm " + header)
        except:
            print("no file")
        print("removing " + header)
        
        output_dir = "/".join(output[0].split("/")[:-1])
        iterator = 0
        
        for filename in input:
            file_list = glob.glob("_".join(filename.split("_")[:-1]) + "_*.tsv")
            
            
            for file in file_list:
                rate = pd.read_csv(file, sep = "\t")
            
                rate.to_csv(output_dir + "/chunk_" + str(wildcards.chunksize) + "_" + str(iterator) + ".tsv", sep = "\t", index = None)
                iterator += 1
                    
            
            
            
rule get_length:
    input:
        os.path.join(scratch_dir, "kl_input/" + file_directory + "{header}_chr_{chrom_all}/chunk_{chunksize}_0.tsv")
    output:
        os.path.join(scratch_dir, "kl_input/"+ file_directory +"{header}_chr_{chrom_all}_chunk_{chunksize}_length.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb_length
    run:
        with Client() as client:
            
            input_filename = "_".join(input[0].split("_")[:-1]) + "_*.tsv"
            
            rate = dd.read_csv(input_filename, sep = "\t")
            
            length = len(rate)
            column_length = len(rate.columns)
            
            f = open(output[0], "w")
            
            f.write(str(length) + "\n")
            f.write(str(column_length) + "\n")
            
            f.close()
        
        
########################################## For running KL analysis #####################################
        
rule run_KL_cov:
    input:
        variants = os.path.join(scratch_dir, "kl_input/" + file_directory + "{header}_chr_{chrom_all}/chunk_{chunksize}_0.tsv"),
        length_file = os.path.join(scratch_dir, "kl_input/" + file_directory +"{header}_chr_{chrom_all}_chunk_{chunksize}_length.tsv"),
        neutral_sfs = os.path.join(KL_data_dir, "whole_genome/allele_freq/adaptive_bins/all.tsv"),
#         neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all.tsv",
#         neutral_sfs = KL_data_dir + "/whole_genome/neutral/5_bins/all_original.tsv"
    output:
        os.path.join(KL_data_dir, "raklette_output/"+ file_directory +"{header}_chr_{chrom_all}_chunksize_{chunksize}_covonly_lr_{learning_rate}_gamma_{gamma}_epoch_{epoch}_covprior_{cov_prior}.pkl")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=10,
        mem_mb=get_mem_mb_raklette
    run:
        n_covs = 1
        
        input_filename = input.variants
        input_directory = "/".join(input_filename.split("/")[:-1]) + "/"
        output_filename = output[0]
        neutral_sfs_filename = input.neutral_sfs
        
        df = pd.read_csv(input.length_file, sep = "\t", header = None)
        nb_samples = df[0][0]
        nb_features = df[0][1] - 2
        
        print("number of samples: " + str(nb_samples))
        
        if nb_samples == 0:
            f = open(output_filename, "w")
            f.write("no sample")
            f.close()
        else:        
            with open(input.variants) as f:
                first_line = f.readline()
            header = first_line.split("\t")
            
            chunksize = int(wildcards.chunksize)

            print("number of chunks " + str(nb_samples/chunksize))
            
            dataset = TSVDataset(input_directory, chunksize=chunksize, nb_samples = nb_samples, header_all = header, features = header)
            loader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False)

            num_epochs = int(wildcards.epoch)
            cov_prior = float(wildcards.cov_prior)
            
            #lets run raklette
            run_raklette_cov(loader, nb_features, num_epochs, neutral_sfs_filename, output_filename, 
                         float(wildcards.learning_rate), float(wildcards.gamma), 
                             cov_sigma_prior = torch.tensor(cov_prior, dtype=torch.float32))