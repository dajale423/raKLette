import sys
sys.path.insert(0,'/home/djl34/kl_git/scripts')
from snakefile_imports import *

n = 1

## function for allocating memory into 
def get_mem_mb(wildcards, attempt):
    return 10000 + (attempt + n) * 30000
    
print(chrom_set)

cwd = os.getcwd()
file_directory = "/".join(cwd.split("/")[-2:]) + "/"

# include: "/home/djl34/kl_git/preprocessing/whole_genome/Snakefile"
# include: "/home/djl34/kl_git/results/method_validation/Snakefile"

##############################################################################################################################   
## for epigenetic features
transform_list = ["sqrt", "log_plus_one", "no_common", "no_transform"]

input_list = [os.path.join(KL_data_dir, f"results/footprints/zscore/AC/{transformation}/{chrom}.tsv") for transformation in transform_list for chrom in chrom_set]

filename_list = [os.path.join(scratch_dir, "results/footprints/ac/{chrom}.tsv")]

# filename_list = [os.path.join(scratch_dir, "results/footprints/denovo/{chrom}.tsv")]

# input_list = [filename.replace("{chrom}", str(chromosome)) for filename in filename_list for chromosome in chrom_set]

rule all:
    input:
        os.path.join(KL_data_dir, "whole_genome/neutral/expected_AC/log_plus_one.tsv"),
        os.path.join(KL_data_dir, "whole_genome/neutral/expected_AC/sqrt.tsv"),
        os.path.join(KL_data_dir, "whole_genome/neutral/expected_AC/no_transform.tsv"),
        input_list,
        # [os.path.join(scratch_dir, f"results/footprints/top/top_region_{rank}.tsv")  for rank in [1000, 5000]]
        # [os.path.join(scratch_dir, f"results/footprints/top/top_{rank}.tsv") for rank in [1000, 5000]]

################################################# creating data for results ##################################################

# create data for footprints
rule make_footprints:
    input:
        os.path.join(KL_data_dir, "whole_genome/footprints/{chrom}/_metadata")
    output:
        os.path.join(scratch_dir, "results/footprints/split/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            filename = input[0]
            
            rate = dd.read_parquet("/".join(filename.split("/")[:-1]) + "/")
            rate["footprint_mean_signal"] = rate["footprint_mean_signal"].fillna(0)
            rate_footprints = rate[(rate["footprint_mean_signal"] > 0)]
            rate_footprints.to_csv(output[0], sep = "\t", single_file = True)

rule add_AC:
    input:
        rate = os.path.join(scratch_dir, "results/footprints/split/{chrom}.tsv"),
        ac = os.path.join(KL_data_dir, "whole_genome/freq_bins/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "results/footprints/ac/{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object', 'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'})
            feature = dd.read_parquet("/".join(input.ac.split("/")[:-1]) + "/")
            rate = rate.drop(['mu'], axis=1)

            rate["Pos"] = rate["Pos"].astype("Int64")
            
            rate = rate.merge(feature, on = ['Pos', 'Allele_ref', 'Allele'], how = "left")
            rate.to_csv(output[0], sep = "\t", single_file = True, index = None)


rule shorten:
    input:
        rate = os.path.join(scratch_dir, "results/footprints/ac/{chrom}.tsv"),
    output:
        os.path.join(KL_data_dir, "results/footprints/data/{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=3000
    run:
        with Client() as client:
            
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object', 'DHS_tissue': 'object', 'footprint_motif_clusters': 'object'})
            keep_columns = ["Pos", "Allele_ref", "Allele", "footprint_mean_signal", "footprint_samples", "footprint_motif_clusters", "footprint_identifier", "mu", "mu_index", "AN_total_interpolate", "AC_total", "AF"]
            
            rate[keep_columns].to_csv(output[0], sep = "\t", single_file = True, index = None)

rule add_denovo:
    input:
        rate = os.path.join(scratch_dir, "results/footprints/ac/{chrom}.tsv"),
        denovo = os.path.join(KL_data_dir, "de_novo/an_etal/aat6576_table-s2.csv"),
        denovo2 = os.path.join(KL_data_dir, "de_novo/halldorsson_etal/aau1043_datas5_revision1.tsv"),
    output:
        os.path.join(scratch_dir, "results/footprints/denovo/{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object'})
            
            df = pd.read_csv(input.denovo)
            
            df = df.rename({"Alt": "Allele"}, axis = 1)
            
            df = df[df["Type"] == "SNV"]
            df = df[df["Chr"] == "chr" + wildcards.chrom]
            
            df_case = df[df["Pheno"] == "case"]
            df_control = df[df["Pheno"] == "control"]
            
            df_case["denovo_an_case"] = 1
            df_control["denovo_an_control"] = 1
            
            rate = rate.merge(df_case[["Pos", "Allele", "denovo_an_case"]], on = ["Pos", "Allele"], how = "left")
            rate = rate.merge(df_control[["Pos", "Allele", "denovo_an_control"]], on = ["Pos", "Allele"], how = "left")

            #for second control_file
            df = pd.read_csv(input.denovo2, sep = "\t", comment = "#")
            df = df.rename({"Alt": "Allele"}, axis = 1)
            df = df[df["Chr"] == f"chr{wildcards.chrom}"]

            df["denovo_halldorsson_control"] = 1
            rate = rate.merge(df[["Pos", "Allele", "denovo_halldorsson_control"]], on = ["Pos", "Allele"], how = "left")
            
            rate.to_csv(output[0], sep = "\t", single_file = True, index = None)
            
rule add_zoonomia_footprints:
    input:
        rate = os.path.join(scratch_dir, "results/footprints/split/{chrom}.tsv"),
        zoo = os.path.join(KL_data_dir, "zoonomia/cactus241way.phyloP_chr{chrom}.wig")
    output:
        os.path.join(scratch_dir, "results/footprints/zoonomia/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        from dask.distributed import Client

        with Client() as client:
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object'})

            zoo = pd.read_csv(input.zoo, sep = "\t", names = ["Pos", "phyloP"], dtype={'Pos': 'int', 'phyloP': 'float64'})
#             zoo = zoo.repartition(partition_size="1GB")

            rate = rate.merge(zoo, on = "Pos", how = "left")

            rate["phyloP"] = rate["phyloP"].fillna(0)
            rate["phyloP_pos"] = rate["phyloP"].where(rate["phyloP"] > 0, 0)

            rate = rate.repartition(partition_size="3GB")

            rate.to_csv(output[0], sep = "\t", single_file = True, index = None)

# rule filter_columns:
#     input:
#         rate = os.path.join(scratch_dir, "results/footprints/zoonomia/{chrom}.tsv"),
#     output:
#         os.path.join(KL_data_dir, "results/footprints/sites_table/{chrom}.tsv"),
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         from dask.distributed import Client

#         with Client() as client:
#             rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object'})

#             rate = rate[["mu", "mu_index", "AN_total_interpolate", "AC_total", "footprint_identifier", "denovo_an_case", "denovo_an_control", "phyloP"]]
            
#             rate.to_csv(output[0], sep = "\t", single_file = True, index = None)


################################################# play around with AC distribution ##################################################

## calculate expected AC
# mark which regions are exonic
rule calculate_expected_AC:
    input:
        rate = os.path.join(KL_data_dir, "whole_genome/neutral/{chrom}/_metadata"),
    output:
        os.path.join(scratch_dir, "whole_genome/neutral/AC_total/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-0:30",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:        
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            df = rate[["mu", "AC_total", 'AF', "Neutral"]]
            df = df[df["Neutral"] == 1]
            df = df.compute()
                        
            df = df[["mu", "AF", "AC_total"]]
            
            df.to_csv(output[0], sep = "\t", index = None)

rule AC_transformation:
    input:
        [os.path.join(scratch_dir, f"whole_genome/neutral/AC_total/{chrom}.tsv") for chrom in all_chrom_set]
    output:
        os.path.join(KL_data_dir, "whole_genome/neutral/expected_AC/{transform}.tsv"),
    resources:
        partition="short",
        runtime="0-0:30",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:        
            rate = dd.read_csv(input, sep = "\t")
            
            df = rate.compute()

            if wildcards.transform == "no_transform":
                print("no_transform")
            elif wildcards.transform == "no_common":
                df = df[df["AF"] < 0.05]
            elif wildcards.transform == "sqrt":
                df["AC_total"] = df["AC_total"].apply(lambda x: np.sqrt(x))
            elif wildcards.transform == "log_plus_one":
                df["AC_total"] = df["AC_total"].apply(lambda x: np.log(x+1))


            df = df.drop(["AF"], axis = 1)
            
            df_group = pd.DataFrame(df.groupby("mu").mean())
            df_group["AC_var"] = df.groupby("mu").var()
            df_group["sites"] = df.groupby("mu").size()
            df_group = df_group.reset_index()
            
            df_group = df_group.rename({"AC_total": "expected_AC"}, axis = 1)
            
            df_group.to_csv(output[0], sep = "\t", index = None)



rule zscore_AC_transformation:
    input:
        rate = os.path.join(KL_data_dir, "results/footprints/data/{chrom}.tsv"),
        ac = os.path.join(KL_data_dir, "whole_genome/neutral/expected_AC/{transformation}.tsv")
    output:
        os.path.join(KL_data_dir, "results/footprints/zscore/AC/{transformation}/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-0:10",
        cpus_per_task=5,
        mem_mb=10000
    run:
        with Client() as client:
            
            filename = input.rate
            
            ddf = dd.read_csv(filename, sep = "\t", dtype={'Spliceai_info': 'object'})
                                    
            df = pd.read_csv(input.ac, sep = "\t")
            df = df.rename({"AC_total": "expected_AC"}, axis = 1)
            
            ddf = ddf.merge(df[["mu", 'expected_AC', 'AC_var']], on = "mu", how = "left")
            
            df_groupby = ddf[["footprint_identifier", "mu", 'expected_AC', 'AC_var', "AC_total"]].groupby("footprint_identifier").sum().compute()
            df_groupby["AN_mean"] = ddf[["footprint_identifier", "AN_total_interpolate"]].groupby("footprint_identifier").mean().compute()
            df_groupby["sites"] = ddf[["footprint_identifier", "mu", "AC_total"]].groupby("footprint_identifier").size().compute()
            
            df_groupby.to_csv(output[0], sep = "\t")

################################################# calculate various Zscores ##################################################
# create data for neutral sites
rule zscore_binary:
    input:
        rate = os.path.join(KL_data_dir, "results/footprints/data/{chrom}.tsv"),
        neutral = os.path.join(KL_data_dir, "whole_genome/freq_bins/freq_bin_9_all.tsv"),
    output:
        os.path.join(KL_data_dir, "results/footprints/zscore/binary/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            filename = input.rate
            
            ddf = dd.read_csv(filename, sep = "\t", dtype={'Spliceai_info': 'object'})
            
            # ## filter common variants
            # ddf = ddf[ddf["AF"] < 0.05]

            #from neutral SFS calculate probably of being polymorphic
            neutral = pd.read_csv(input.neutral, sep = "\t")
            neutral = neutral.rename({"0.0": "monomorphic"}, axis = 1)
            neutral["polymorphic"] = 1 - neutral["monomorphic"]
            neutral_dict = dict(zip(neutral.index, neutral.polymorphic))
            
            ddf["polymorphic"] = ddf["AC_total"] > 0
            ddf["polymorphic"] = ddf["polymorphic"].astype(int)
            ddf["polymorphic_expected"] = ddf['mu_index'].map(neutral_dict)

            df_groupby = ddf[["footprint_identifier", "polymorphic", 'polymorphic_expected']].groupby("footprint_identifier").sum().compute()
            df_groupby["sites"] = ddf[["footprint_identifier", "mu", "AC_total"]].groupby("footprint_identifier").size().compute()
            
            df_groupby.to_csv(output[0], sep = "\t")


# rule zscore_AC:
#     input:
#         rate = os.path.join(scratch_dir, "results/footprints/split/{chrom}.tsv"),
#         ac = os.path.join(KL_data_dir, "whole_genome/neutral/expected_AC/all.tsv")
#     output:
#         os.path.join(KL_data_dir, "results/footprints/zscore/AC/{chrom}.tsv"),
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         with Client() as client:
            
#             filename = input.rate
            
#             ddf = dd.read_csv(filename, sep = "\t", dtype={'Spliceai_info': 'object'})
            
#             ## filter common variants
#             ddf = ddf[ddf["AF"] < 0.05]
                        
#             df = pd.read_csv(input.ac, sep = "\t")
#             df = df.rename({"AC_total": "expected_AC"}, axis = 1)
            
#             ddf = ddf.merge(df[["mu", 'expected_AC', 'AC_var']], on = "mu", how = "left")
            
#             df_groupby = ddf[["footprint_identifier", "mu", 'expected_AC', 'AC_var', "AC_total"]].groupby("footprint_identifier").sum().compute()
#             df_groupby["AN_mean"] = ddf[["footprint_identifier", "AN_total_interpolate"]].groupby("footprint_identifier").mean().compute()
#             df_groupby["sites"] = ddf[["footprint_identifier", "mu", "AC_total"]].groupby("footprint_identifier").size().compute()
            
#             df_groupby.to_csv(output[0], sep = "\t")

rule zscore_KL:
    input:
        rate = os.path.join(KL_data_dir, "results/footprints/data/{chrom}.tsv"),
        neutral = os.path.join(KL_data_dir, "whole_genome/freq_bins/freq_bin_9_all.tsv"),
    output:
        os.path.join(KL_data_dir, "results/footprints/zscore/kl/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-0:20",
        cpus_per_task=5,
        mem_mb=10000
    run:
        with Client() as client:
            
            filename = input.rate
            
            ddf = dd.read_csv(filename, sep = "\t", dtype={'Spliceai_info': 'object'})
            
            # ## filter common variants
            # ddf = ddf[ddf["AF"] < 0.05]

            #from neutral SFS calculate probably of being polymorphic
            neutral = pd.read_csv(input.neutral, sep = "\t")
            neutral = neutral.rename({"0.0": "monomorphic"}, axis = 1)
            neutral["polymorphic"] = 1 - neutral["monomorphic"]
            neutral_dict = dict(zip(neutral.index, neutral.polymorphic))
            
            ddf["polymorphic"] = 1
            ddf["polymorphic"] = ddf["polymorphic"].where(ddf["AC_total"] > 0, 0)
            ddf["polymorphic_expected"] = ddf['mu_index'].map(neutral_dict)

            df_groupby = ddf[["footprint_identifier", "polymorphic", 'polymorphic_expected']].groupby("footprint_identifier").sum().compute()
            df_groupby["sites"] = ddf[["footprint_identifier", "mu", "AC_total"]].groupby("footprint_identifier").size().compute()
            
            df_groupby.to_csv(output[0], sep = "\t")

################################################# get validation data ##################################################
## calculate phyloP for all of the footprints
## first output for output without filtering
## second output for output after filtering out for AF > 0.05
rule get_phyloP_for_footprints:
    input:
        os.path.join(scratch_dir, "results/footprints/zoonomia/{chrom}.tsv")
    output:
        os.path.join(KL_data_dir, "results/footprints/phyloP/{chrom}.tsv"),        
        os.path.join(KL_data_dir, "results/footprints/phyloP/filtered_common_{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            filename = input[0]
            
            ddf = dd.read_csv(filename, sep = "\t", dtype={'Spliceai_info': 'object'})
            
            df_groupby = ddf[["footprint_identifier", "phyloP", "phyloP_pos"]].groupby("footprint_identifier").mean().compute()
            df_groupby["phyloP_max"] = ddf[["footprint_identifier", "phyloP"]].groupby("footprint_identifier").max().compute()
            df_groupby["phyloP_median"] = ddf[["footprint_identifier", "phyloP"]].groupby("footprint_identifier").median().compute()
            
            df_groupby.to_csv(output[0], sep = "\t")

            ## filter common variants
            ddf = ddf[ddf["AF"] < 0.05]
            
            df_groupby = ddf[["footprint_identifier", "phyloP", "phyloP_pos"]].groupby("footprint_identifier").mean().compute()
            df_groupby["phyloP_max"] = ddf[["footprint_identifier", "phyloP"]].groupby("footprint_identifier").max().compute()
            df_groupby["phyloP_median"] = ddf[["footprint_identifier", "phyloP"]].groupby("footprint_identifier").median().compute()
            
            df_groupby.to_csv(output[1], sep = "\t")

## calculate de novo for all of the footprints
rule get_denovo:
    input:
        rate = os.path.join(scratch_dir, "results/footprints/denovo/{chrom}.tsv"),
        ac = os.path.join(KL_data_dir, "whole_genome/neutral/expected_AC/all.tsv")
    output:
        os.path.join(KL_data_dir, "results/footprints/denovo/{chrom}.tsv"),
        os.path.join(KL_data_dir, "results/footprints/denovo/filtered_common_{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            filename = input.rate
            
            ddf = dd.read_csv(filename, sep = "\t", dtype={'Spliceai_info': 'object'})

            ddf["denovo_an_case"] = ddf["denovo_an_case"].fillna(0)
            ddf["denovo_an_control"] = ddf["denovo_an_control"].fillna(0)
            ddf["denovo_halldorsson_contorl"] = ddf["denovo_halldorsson_control"].fillna(0)
            
            df_groupby = ddf[["footprint_identifier", "denovo_an_case", "denovo_an_control", "denovo_halldorsson_control",]].groupby("footprint_identifier").sum().compute()

            df_groupby.to_csv(output[0], sep = "\t")
            
            ## filter common variants
            ddf = ddf[ddf["AF"] < 0.05]
            df_groupby = ddf[["footprint_identifier", "denovo_an_case", "denovo_an_control", "denovo_halldorsson_control"]].groupby("footprint_identifier").sum().compute()

            df_groupby.to_csv(output[1], sep = "\t")

################################################# give output of sites ##################################################
# create data for footprints
rule make_footprints_sites:
    input:
        rate = os.path.join(scratch_dir, "results/footprints/split/{chrom}.tsv"),
        footprints = "/home/djl34/kl_git/results/footprints/top_{rank}_footprints.tsv"
    output:
        os.path.join(scratch_dir, "results/footprints/top/{chrom}_top_{rank}.tsv"),
        os.path.join(scratch_dir, "results/footprints/top/{chrom}_non_top_{rank}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=30000
    run:
        with Client() as client:
            rate = dd.read_csv(input.rate, sep = "\t", dtype={'Spliceai_info': 'object'})
            df = pd.read_csv(input.footprints, sep = "\t", dtype={'Spliceai_info': 'object'})
            footprint_list = list(df["footprint_identifier"])
            
            rate = rate[["Pos", "Allele_ref", "Allele", "footprint_identifier"]]
            rate["Chrom"] = wildcards.chrom
            rate_filtered = rate[rate["footprint_identifier"].isin(footprint_list)]
            rate_filtered_out = rate[~rate["footprint_identifier"].isin(footprint_list)]
            
            rate_filtered.to_csv(output[0], sep = "\t", index = None, single_file = True)
            rate_filtered_out.to_csv(output[1], sep = "\t", index = None, single_file = True)

rule sites_combined_by_chrom:
    input:
        top = [os.path.join(scratch_dir, f"results/footprints/top/{chrom}_top_{{rank}}.tsv") for chrom in all_chrom_set],
        bottom = [os.path.join(scratch_dir, f"results/footprints/top/{chrom}_non_top_{{rank}}.tsv") for chrom in all_chrom_set]
    output:
        os.path.join(scratch_dir, "results/footprints/top/top_{rank}.tsv"),
        os.path.join(scratch_dir, "results/footprints/top/non_top_{rank}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=30000
    run:
        with Client() as client:
            rate = dd.read_csv(input.top, sep = "\t", dtype={'Spliceai_info': 'object'})
            
            rate.to_csv(output[0], sep = "\t", index = None, single_file = True)

            rate = dd.read_csv(input.bottom, sep = "\t", dtype={'Spliceai_info': 'object'})
            
            rate.to_csv(output[1], sep = "\t", index = None, single_file = True)

rule get_region:
    input:
        top = "/home/djl34/kl_git/results/footprints/top_{rank}_footprints.tsv",
        footprints = os.path.join(KL_data_dir, "footprints/consensus_footprints_and_collapsed_motifs_hg38.bed.gz")
    output:
        os.path.join(scratch_dir, "results/footprints/top/top_region_{rank}.tsv"),
        os.path.join(scratch_dir, "results/footprints/top/non_top_region_{rank}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=30000
    run:
        with Client() as client:
            df = pd.read_csv(input.top, sep = "\t", dtype={'Spliceai_info': 'object'})
            footprint_list = list(df["footprint_identifier"])

            fp_header = ['Chrom', 'start', 'end', 'identifier', 'mean_signal', 'num_samples', 'num_fps', 'width', 'summit_pos', 'core_start', 'core_end', 'motif_clusters']
            fp = pd.read_csv(input.footprints, sep = "\t", names = fp_header)
            
            fp_top = fp[fp["identifier"].isin(footprint_list)]
            fp_bottom = fp[~fp["identifier"].isin(footprint_list)]
            
            fp_top.to_csv(output[0], sep = "\t", index = None)
            fp_bottom.to_csv(output[1], sep = "\t", index = None)
